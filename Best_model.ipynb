{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aekQfqyGK1-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "import time\n",
        "from project_utilities import ValueSet, Loss, efficiency\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_idx, val_set_idx=train_test_split(list(range(1,80)),test_size=15)\n",
        "len(train_set_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKfQ-8MuIKpf",
        "outputId": "5d971c45-e5ee-4356-90c3-7e37a136fe2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = ('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "18tDTMQPILsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DCvO0M4qINFd",
        "outputId": "012f0381-3fd3-407f-c602-d27c7075152e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOSq21UEIOfA",
        "outputId": "2cb62f74-7a3c-4909-d09f-66e2b44f6a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed= 123\n",
        "NUM_EPOCHS=6\n",
        "LEARNING_RATE=0.001\n",
        "BATCH_SIZE=128"
      ],
      "metadata": {
        "id": "jVTOZ86cIP4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_h6e4zuIRuB",
        "outputId": "c34d5f21-3b35-4853-b2fd-ae00dfd5f6e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/Project/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdRShG8SITev",
        "outputId": "fac6a95d-c805-40a8-9d12-5fa784b8fe46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Project/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(torch.utils.data.Dataset): \n",
        "  def __init__(self, setID): \n",
        "        'Initialization' \n",
        "        npz_files_content = np.load(\"./Set_\"+str(setID)+\".npz\")  \n",
        "         \n",
        "        self.X_set = torch.tensor(npz_files_content['X'])    \n",
        "        self.y_set = torch.tensor(npz_files_content['y']) \n",
        "  def __len__(self): \n",
        "        'Denotes the total number of samples' \n",
        "        return len(self.y_set) \n",
        "  def __getitem__(self, index): \n",
        "        'Generates one sample of data' \n",
        "        # Select sample \n",
        "        X = self.X_set[index] \n",
        "        y = self.y_set[index] \n",
        "        return X, y"
      ],
      "metadata": {
        "id": "3yybtI4mIYzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################\n",
        "##### Training and evaluation wrappers\n",
        "###################################################\n",
        "def train(model, num_epochs,\n",
        "          learning_rate=0.01, seed=123, batch_size=128):\n",
        "          \n",
        "  \n",
        "    # print(1)\n",
        "    cost = []\n",
        "    torch.manual_seed(seed)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=0.1,mode='max',verbose=True)\n",
        "    for e in range(1,num_epochs):\n",
        "      # print(2)\n",
        "      batch_num = 0\n",
        "      for setID in train_set_idx:\n",
        "          train_set = MyDataset(setID) # removed + 1\n",
        "          train_generator = torch.utils.data.DataLoader(train_set,batch_size=batch_size,shuffle=True,drop_last=True)\n",
        "          # print(setID)\n",
        "          for X_train, y_train in train_generator:\n",
        "            X_train=X_train.to(DEVICE)\n",
        "            y_train=y_train.to(DEVICE)\n",
        "            batch_num = batch_num + 1\n",
        "            #### Compute outputs ####\n",
        "            yhat = model(X_train)\n",
        "            # print(3)\n",
        "            loss = loss_model.forward(yhat, y_train)\n",
        "            #### Reset gradients from previous iteration ####\n",
        "            optimizer.zero_grad()\n",
        "            #### Compute gradients ####\n",
        "            loss.backward()\n",
        "            #### Update weights ####\n",
        "            optimizer.step()\n",
        "            # print(4)\n",
        "            #### Logging ####\n",
        "            with torch.no_grad():\n",
        "                yhat = model.forward(X_train)\n",
        "                curr_loss = loss_model.forward(yhat, y_train)\n",
        "                print('Epoch ID: %d ' % e, end=\"\")\n",
        "                print('  Set ID: %d' % setID, end=\"\")\n",
        "                print('  Batch ID: %d' % batch_num, end=\"\")\n",
        "                print(' | Loss: %.5f' % curr_loss)\n",
        "                cost.append(curr_loss)\n",
        "    return cost\n"
      ],
      "metadata": {
        "id": "SPoj7SI3If62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trim(torch.nn.Module):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :16000]"
      ],
      "metadata": {
        "id": "MxHmReJ_l9qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Resnet10(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Resnet10, self).__init__()\n",
        "\n",
        "        self.block_1 = torch.nn.Sequential(\n",
        "                torch.nn.Conv1d(in_channels=1,\n",
        "                                out_channels=2,\n",
        "                                kernel_size=2,\n",
        "                                stride=1,\n",
        "                                padding=0),\n",
        "                torch.nn.BatchNorm1d(2),\n",
        "                # torch.nn.MaxPool2d(kernel_size=2),\n",
        "                torch.nn.ReLU(inplace=True),\n",
        "\n",
        "                torch.nn.Conv1d(in_channels=2,\n",
        "                                out_channels=1,\n",
        "                                kernel_size=1,\n",
        "                                stride=1,\n",
        "                                padding=1),\n",
        "                Trim(),                \n",
        "                torch.nn.BatchNorm1d(1)\n",
        "                \n",
        "        )\n",
        "        self.block_2 = torch.nn.Sequential(\n",
        "                torch.nn.Conv1d(in_channels=1,\n",
        "                                out_channels=2,\n",
        "                                kernel_size=2,\n",
        "                                stride=1,\n",
        "                                padding=0),\n",
        "                torch.nn.BatchNorm1d(2),\n",
        "                # torch.nn.MaxPool2d(kernel_size=2),\n",
        "                torch.nn.ReLU(inplace=True),\n",
        "\n",
        "                torch.nn.Conv1d(in_channels=2,\n",
        "                                out_channels=1,\n",
        "                                kernel_size=2,\n",
        "                                stride=1,\n",
        "                                padding=1),\n",
        "                torch.nn.BatchNorm1d(1)\n",
        "        )\n",
        "        self.block_3 = torch.nn.Sequential(\n",
        "                torch.nn.Conv1d(in_channels=1,\n",
        "                                out_channels=2,\n",
        "                                kernel_size=1,\n",
        "                                stride=1,\n",
        "                                padding=0),\n",
        "                torch.nn.BatchNorm1d(2),\n",
        "                torch.nn.MaxPool1d(kernel_size=2),\n",
        "                torch.nn.ReLU(inplace=True),\n",
        "                torch.nn.Conv1d(in_channels=2,\n",
        "                                out_channels=1,\n",
        "                                kernel_size=2,\n",
        "                                stride=1,\n",
        "                                padding=1),\n",
        "                torch.nn.MaxPool1d(kernel_size=2),\n",
        "                torch.nn.BatchNorm1d(1)\n",
        "        )\n",
        "        # for m in self.modules():\n",
        "        #   if isinstance(m, torch.nn.Linear):\n",
        "        #     torch.nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "        #     if m.bias is not None:\n",
        "        #       m.bias.detach().zero_()\n",
        "          \n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        n,h,w=x.shape\n",
        "        x=x.view(n,1,h*w)\n",
        "        # print(x.shape)\n",
        "        shortcut = x\n",
        "        x = self.block_1(x)\n",
        "        # print(x.shape)\n",
        "        x = torch.nn.functional.relu(x + shortcut)\n",
        "        # print(x.shape)\n",
        "        n,c,w=x.shape\n",
        "        x=x.view(n,1,w)\n",
        "        shortcut = x\n",
        "        x = self.block_2(x)\n",
        "        # print(x.shape)\n",
        "        x = torch.nn.functional.relu(x + shortcut)\n",
        "        # print(x.shape)\n",
        "        x=self.block_3(x)\n",
        "        logits=torch.nn.functional.relu(x)\n",
        "        return logits.view(-1, 4000)\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "JSfw3sH2FimV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Resnet10()\n",
        "model=model.to(DEVICE)\n",
        "loss_model = Loss(0.00001)\n",
        "cost = train(model,\n",
        "             num_epochs=4,\n",
        "             learning_rate=0.001,\n",
        "             seed=123, batch_size=1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_esJ8jfOGNfs",
        "outputId": "5b7b6ae8-7fd2-450c-cfaf-4f601143ad2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 1   Set ID: 34  Batch ID: 1 | Loss: 0.74637\n",
            "Epoch ID: 1   Set ID: 34  Batch ID: 2 | Loss: 0.72212\n",
            "Epoch ID: 1   Set ID: 34  Batch ID: 3 | Loss: 0.73849\n",
            "Epoch ID: 1   Set ID: 34  Batch ID: 4 | Loss: 0.72256\n",
            "Epoch ID: 1   Set ID: 79  Batch ID: 5 | Loss: 0.71264\n",
            "Epoch ID: 1   Set ID: 79  Batch ID: 6 | Loss: 0.70491\n",
            "Epoch ID: 1   Set ID: 79  Batch ID: 7 | Loss: 0.70257\n",
            "Epoch ID: 1   Set ID: 79  Batch ID: 8 | Loss: 0.70231\n",
            "Epoch ID: 1   Set ID: 55  Batch ID: 9 | Loss: 0.66820\n",
            "Epoch ID: 1   Set ID: 55  Batch ID: 10 | Loss: 0.67136\n",
            "Epoch ID: 1   Set ID: 55  Batch ID: 11 | Loss: 0.65955\n",
            "Epoch ID: 1   Set ID: 55  Batch ID: 12 | Loss: 0.63238\n",
            "Epoch ID: 1   Set ID: 60  Batch ID: 13 | Loss: 0.63194\n",
            "Epoch ID: 1   Set ID: 60  Batch ID: 14 | Loss: 0.61416\n",
            "Epoch ID: 1   Set ID: 60  Batch ID: 15 | Loss: 0.62952\n",
            "Epoch ID: 1   Set ID: 60  Batch ID: 16 | Loss: 0.61189\n",
            "Epoch ID: 1   Set ID: 27  Batch ID: 17 | Loss: 0.60833\n",
            "Epoch ID: 1   Set ID: 27  Batch ID: 18 | Loss: 0.61464\n",
            "Epoch ID: 1   Set ID: 27  Batch ID: 19 | Loss: 0.60246\n",
            "Epoch ID: 1   Set ID: 27  Batch ID: 20 | Loss: 0.59669\n",
            "Epoch ID: 1   Set ID: 51  Batch ID: 21 | Loss: 0.58784\n",
            "Epoch ID: 1   Set ID: 51  Batch ID: 22 | Loss: 0.57525\n",
            "Epoch ID: 1   Set ID: 51  Batch ID: 23 | Loss: 0.57815\n",
            "Epoch ID: 1   Set ID: 51  Batch ID: 24 | Loss: 0.55885\n",
            "Epoch ID: 1   Set ID: 62  Batch ID: 25 | Loss: 0.55483\n",
            "Epoch ID: 1   Set ID: 62  Batch ID: 26 | Loss: 0.55230\n",
            "Epoch ID: 1   Set ID: 62  Batch ID: 27 | Loss: 0.54107\n",
            "Epoch ID: 1   Set ID: 62  Batch ID: 28 | Loss: 0.53336\n",
            "Epoch ID: 1   Set ID: 75  Batch ID: 29 | Loss: 0.52753\n",
            "Epoch ID: 1   Set ID: 75  Batch ID: 30 | Loss: 0.53187\n",
            "Epoch ID: 1   Set ID: 75  Batch ID: 31 | Loss: 0.51358\n",
            "Epoch ID: 1   Set ID: 75  Batch ID: 32 | Loss: 0.51974\n",
            "Epoch ID: 1   Set ID: 21  Batch ID: 33 | Loss: 0.52159\n",
            "Epoch ID: 1   Set ID: 21  Batch ID: 34 | Loss: 0.51850\n",
            "Epoch ID: 1   Set ID: 21  Batch ID: 35 | Loss: 0.52111\n",
            "Epoch ID: 1   Set ID: 21  Batch ID: 36 | Loss: 0.51494\n",
            "Epoch ID: 1   Set ID: 4  Batch ID: 37 | Loss: 0.51682\n",
            "Epoch ID: 1   Set ID: 4  Batch ID: 38 | Loss: 0.51849\n",
            "Epoch ID: 1   Set ID: 4  Batch ID: 39 | Loss: 0.51754\n",
            "Epoch ID: 1   Set ID: 4  Batch ID: 40 | Loss: 0.51241\n",
            "Epoch ID: 1   Set ID: 76  Batch ID: 41 | Loss: 0.50864\n",
            "Epoch ID: 1   Set ID: 76  Batch ID: 42 | Loss: 0.51202\n",
            "Epoch ID: 1   Set ID: 76  Batch ID: 43 | Loss: 0.50083\n",
            "Epoch ID: 1   Set ID: 76  Batch ID: 44 | Loss: 0.50828\n",
            "Epoch ID: 1   Set ID: 43  Batch ID: 45 | Loss: 0.49958\n",
            "Epoch ID: 1   Set ID: 43  Batch ID: 46 | Loss: 0.49692\n",
            "Epoch ID: 1   Set ID: 43  Batch ID: 47 | Loss: 0.50416\n",
            "Epoch ID: 1   Set ID: 43  Batch ID: 48 | Loss: 0.50582\n",
            "Epoch ID: 1   Set ID: 17  Batch ID: 49 | Loss: 0.50109\n",
            "Epoch ID: 1   Set ID: 17  Batch ID: 50 | Loss: 0.49633\n",
            "Epoch ID: 1   Set ID: 17  Batch ID: 51 | Loss: 0.49417\n",
            "Epoch ID: 1   Set ID: 17  Batch ID: 52 | Loss: 0.49040\n",
            "Epoch ID: 1   Set ID: 36  Batch ID: 53 | Loss: 0.48887\n",
            "Epoch ID: 1   Set ID: 36  Batch ID: 54 | Loss: 0.49169\n",
            "Epoch ID: 1   Set ID: 36  Batch ID: 55 | Loss: 0.49666\n",
            "Epoch ID: 1   Set ID: 36  Batch ID: 56 | Loss: 0.49180\n",
            "Epoch ID: 1   Set ID: 59  Batch ID: 57 | Loss: 0.49116\n",
            "Epoch ID: 1   Set ID: 59  Batch ID: 58 | Loss: 0.48828\n",
            "Epoch ID: 1   Set ID: 59  Batch ID: 59 | Loss: 0.48252\n",
            "Epoch ID: 1   Set ID: 59  Batch ID: 60 | Loss: 0.46451\n",
            "Epoch ID: 1   Set ID: 9  Batch ID: 61 | Loss: 0.46743\n",
            "Epoch ID: 1   Set ID: 9  Batch ID: 62 | Loss: 0.45409\n",
            "Epoch ID: 1   Set ID: 9  Batch ID: 63 | Loss: 0.45771\n",
            "Epoch ID: 1   Set ID: 9  Batch ID: 64 | Loss: 0.45659\n",
            "Epoch ID: 1   Set ID: 69  Batch ID: 65 | Loss: 0.45582\n",
            "Epoch ID: 1   Set ID: 69  Batch ID: 66 | Loss: 0.44959\n",
            "Epoch ID: 1   Set ID: 69  Batch ID: 67 | Loss: 0.46565\n",
            "Epoch ID: 1   Set ID: 69  Batch ID: 68 | Loss: 0.44628\n",
            "Epoch ID: 1   Set ID: 57  Batch ID: 69 | Loss: 0.45081\n",
            "Epoch ID: 1   Set ID: 57  Batch ID: 70 | Loss: 0.45471\n",
            "Epoch ID: 1   Set ID: 57  Batch ID: 71 | Loss: 0.44415\n",
            "Epoch ID: 1   Set ID: 57  Batch ID: 72 | Loss: 0.44044\n",
            "Epoch ID: 1   Set ID: 56  Batch ID: 73 | Loss: 0.44201\n",
            "Epoch ID: 1   Set ID: 56  Batch ID: 74 | Loss: 0.43886\n",
            "Epoch ID: 1   Set ID: 56  Batch ID: 75 | Loss: 0.44427\n",
            "Epoch ID: 1   Set ID: 56  Batch ID: 76 | Loss: 0.44922\n",
            "Epoch ID: 1   Set ID: 26  Batch ID: 77 | Loss: 0.44586\n",
            "Epoch ID: 1   Set ID: 26  Batch ID: 78 | Loss: 0.43629\n",
            "Epoch ID: 1   Set ID: 26  Batch ID: 79 | Loss: 0.44098\n",
            "Epoch ID: 1   Set ID: 26  Batch ID: 80 | Loss: 0.43699\n",
            "Epoch ID: 1   Set ID: 35  Batch ID: 81 | Loss: 0.44250\n",
            "Epoch ID: 1   Set ID: 35  Batch ID: 82 | Loss: 0.44074\n",
            "Epoch ID: 1   Set ID: 35  Batch ID: 83 | Loss: 0.43514\n",
            "Epoch ID: 1   Set ID: 35  Batch ID: 84 | Loss: 0.44055\n",
            "Epoch ID: 1   Set ID: 44  Batch ID: 85 | Loss: 0.44514\n",
            "Epoch ID: 1   Set ID: 44  Batch ID: 86 | Loss: 0.44494\n",
            "Epoch ID: 1   Set ID: 44  Batch ID: 87 | Loss: 0.43549\n",
            "Epoch ID: 1   Set ID: 44  Batch ID: 88 | Loss: 0.43930\n",
            "Epoch ID: 1   Set ID: 40  Batch ID: 89 | Loss: 0.43557\n",
            "Epoch ID: 1   Set ID: 40  Batch ID: 90 | Loss: 0.43280\n",
            "Epoch ID: 1   Set ID: 40  Batch ID: 91 | Loss: 0.43073\n",
            "Epoch ID: 1   Set ID: 40  Batch ID: 92 | Loss: 0.43129\n",
            "Epoch ID: 1   Set ID: 8  Batch ID: 93 | Loss: 0.43563\n",
            "Epoch ID: 1   Set ID: 8  Batch ID: 94 | Loss: 0.42291\n",
            "Epoch ID: 1   Set ID: 8  Batch ID: 95 | Loss: 0.42319\n",
            "Epoch ID: 1   Set ID: 8  Batch ID: 96 | Loss: 0.42263\n",
            "Epoch ID: 1   Set ID: 31  Batch ID: 97 | Loss: 0.41544\n",
            "Epoch ID: 1   Set ID: 31  Batch ID: 98 | Loss: 0.41273\n",
            "Epoch ID: 1   Set ID: 31  Batch ID: 99 | Loss: 0.41486\n",
            "Epoch ID: 1   Set ID: 31  Batch ID: 100 | Loss: 0.39397\n",
            "Epoch ID: 1   Set ID: 32  Batch ID: 101 | Loss: 0.34849\n",
            "Epoch ID: 1   Set ID: 32  Batch ID: 102 | Loss: 4.68397\n",
            "Epoch ID: 1   Set ID: 32  Batch ID: 103 | Loss: 0.29628\n",
            "Epoch ID: 1   Set ID: 32  Batch ID: 104 | Loss: 0.36145\n",
            "Epoch ID: 1   Set ID: 11  Batch ID: 105 | Loss: 0.37424\n",
            "Epoch ID: 1   Set ID: 11  Batch ID: 106 | Loss: 0.38568\n",
            "Epoch ID: 1   Set ID: 11  Batch ID: 107 | Loss: 0.40333\n",
            "Epoch ID: 1   Set ID: 11  Batch ID: 108 | Loss: 0.40549\n",
            "Epoch ID: 1   Set ID: 45  Batch ID: 109 | Loss: 0.40168\n",
            "Epoch ID: 1   Set ID: 45  Batch ID: 110 | Loss: 0.40696\n",
            "Epoch ID: 1   Set ID: 45  Batch ID: 111 | Loss: 0.40718\n",
            "Epoch ID: 1   Set ID: 45  Batch ID: 112 | Loss: 0.40896\n",
            "Epoch ID: 1   Set ID: 64  Batch ID: 113 | Loss: 0.41234\n",
            "Epoch ID: 1   Set ID: 64  Batch ID: 114 | Loss: 0.41399\n",
            "Epoch ID: 1   Set ID: 64  Batch ID: 115 | Loss: 0.40755\n",
            "Epoch ID: 1   Set ID: 64  Batch ID: 116 | Loss: 0.41159\n",
            "Epoch ID: 1   Set ID: 29  Batch ID: 117 | Loss: 0.42091\n",
            "Epoch ID: 1   Set ID: 29  Batch ID: 118 | Loss: 0.41662\n",
            "Epoch ID: 1   Set ID: 29  Batch ID: 119 | Loss: 0.42611\n",
            "Epoch ID: 1   Set ID: 29  Batch ID: 120 | Loss: 0.41354\n",
            "Epoch ID: 1   Set ID: 5  Batch ID: 121 | Loss: 0.41182\n",
            "Epoch ID: 1   Set ID: 5  Batch ID: 122 | Loss: 0.40261\n",
            "Epoch ID: 1   Set ID: 5  Batch ID: 123 | Loss: 0.41360\n",
            "Epoch ID: 1   Set ID: 5  Batch ID: 124 | Loss: 0.41140\n",
            "Epoch ID: 1   Set ID: 53  Batch ID: 125 | Loss: 0.41061\n",
            "Epoch ID: 1   Set ID: 53  Batch ID: 126 | Loss: 0.40384\n",
            "Epoch ID: 1   Set ID: 53  Batch ID: 127 | Loss: 0.40781\n",
            "Epoch ID: 1   Set ID: 53  Batch ID: 128 | Loss: 0.40540\n",
            "Epoch ID: 1   Set ID: 12  Batch ID: 129 | Loss: 0.42063\n",
            "Epoch ID: 1   Set ID: 12  Batch ID: 130 | Loss: 0.39862\n",
            "Epoch ID: 1   Set ID: 12  Batch ID: 131 | Loss: 0.39520\n",
            "Epoch ID: 1   Set ID: 12  Batch ID: 132 | Loss: 0.39666\n",
            "Epoch ID: 1   Set ID: 39  Batch ID: 133 | Loss: 0.39829\n",
            "Epoch ID: 1   Set ID: 39  Batch ID: 134 | Loss: 0.39954\n",
            "Epoch ID: 1   Set ID: 39  Batch ID: 135 | Loss: 0.38124\n",
            "Epoch ID: 1   Set ID: 39  Batch ID: 136 | Loss: 0.38830\n",
            "Epoch ID: 1   Set ID: 50  Batch ID: 137 | Loss: 0.38620\n",
            "Epoch ID: 1   Set ID: 50  Batch ID: 138 | Loss: 0.39089\n",
            "Epoch ID: 1   Set ID: 50  Batch ID: 139 | Loss: 0.39196\n",
            "Epoch ID: 1   Set ID: 50  Batch ID: 140 | Loss: 0.38155\n",
            "Epoch ID: 1   Set ID: 24  Batch ID: 141 | Loss: 0.39053\n",
            "Epoch ID: 1   Set ID: 24  Batch ID: 142 | Loss: 0.37391\n",
            "Epoch ID: 1   Set ID: 24  Batch ID: 143 | Loss: 0.37758\n",
            "Epoch ID: 1   Set ID: 24  Batch ID: 144 | Loss: 0.37634\n",
            "Epoch ID: 1   Set ID: 25  Batch ID: 145 | Loss: 0.37666\n",
            "Epoch ID: 1   Set ID: 25  Batch ID: 146 | Loss: 0.38356\n",
            "Epoch ID: 1   Set ID: 25  Batch ID: 147 | Loss: 0.38641\n",
            "Epoch ID: 1   Set ID: 25  Batch ID: 148 | Loss: 0.39178\n",
            "Epoch ID: 1   Set ID: 71  Batch ID: 149 | Loss: 0.39174\n",
            "Epoch ID: 1   Set ID: 71  Batch ID: 150 | Loss: 0.39698\n",
            "Epoch ID: 1   Set ID: 71  Batch ID: 151 | Loss: 0.38015\n",
            "Epoch ID: 1   Set ID: 71  Batch ID: 152 | Loss: 0.37039\n",
            "Epoch ID: 1   Set ID: 1  Batch ID: 153 | Loss: 0.36818\n",
            "Epoch ID: 1   Set ID: 1  Batch ID: 154 | Loss: 0.37354\n",
            "Epoch ID: 1   Set ID: 1  Batch ID: 155 | Loss: 0.36935\n",
            "Epoch ID: 1   Set ID: 1  Batch ID: 156 | Loss: 0.37442\n",
            "Epoch ID: 1   Set ID: 66  Batch ID: 157 | Loss: 0.37476\n",
            "Epoch ID: 1   Set ID: 66  Batch ID: 158 | Loss: 0.38012\n",
            "Epoch ID: 1   Set ID: 66  Batch ID: 159 | Loss: 0.38051\n",
            "Epoch ID: 1   Set ID: 66  Batch ID: 160 | Loss: 0.38578\n",
            "Epoch ID: 1   Set ID: 61  Batch ID: 161 | Loss: 0.37387\n",
            "Epoch ID: 1   Set ID: 61  Batch ID: 162 | Loss: 0.36976\n",
            "Epoch ID: 1   Set ID: 61  Batch ID: 163 | Loss: 0.37039\n",
            "Epoch ID: 1   Set ID: 61  Batch ID: 164 | Loss: 0.37483\n",
            "Epoch ID: 1   Set ID: 28  Batch ID: 165 | Loss: 0.37066\n",
            "Epoch ID: 1   Set ID: 28  Batch ID: 166 | Loss: 0.36864\n",
            "Epoch ID: 1   Set ID: 28  Batch ID: 167 | Loss: 0.37121\n",
            "Epoch ID: 1   Set ID: 28  Batch ID: 168 | Loss: 0.37168\n",
            "Epoch ID: 1   Set ID: 73  Batch ID: 169 | Loss: 0.36798\n",
            "Epoch ID: 1   Set ID: 73  Batch ID: 170 | Loss: 0.36824\n",
            "Epoch ID: 1   Set ID: 73  Batch ID: 171 | Loss: 0.37184\n",
            "Epoch ID: 1   Set ID: 73  Batch ID: 172 | Loss: 0.37049\n",
            "Epoch ID: 1   Set ID: 23  Batch ID: 173 | Loss: 0.36957\n",
            "Epoch ID: 1   Set ID: 23  Batch ID: 174 | Loss: 0.37030\n",
            "Epoch ID: 1   Set ID: 23  Batch ID: 175 | Loss: 0.38025\n",
            "Epoch ID: 1   Set ID: 23  Batch ID: 176 | Loss: 0.36207\n",
            "Epoch ID: 1   Set ID: 10  Batch ID: 177 | Loss: 0.37364\n",
            "Epoch ID: 1   Set ID: 10  Batch ID: 178 | Loss: 0.36509\n",
            "Epoch ID: 1   Set ID: 10  Batch ID: 179 | Loss: 0.36656\n",
            "Epoch ID: 1   Set ID: 10  Batch ID: 180 | Loss: 0.37831\n",
            "Epoch ID: 1   Set ID: 30  Batch ID: 181 | Loss: 0.37703\n",
            "Epoch ID: 1   Set ID: 30  Batch ID: 182 | Loss: 0.36443\n",
            "Epoch ID: 1   Set ID: 30  Batch ID: 183 | Loss: 0.37010\n",
            "Epoch ID: 1   Set ID: 30  Batch ID: 184 | Loss: 0.36537\n",
            "Epoch ID: 1   Set ID: 13  Batch ID: 185 | Loss: 0.36106\n",
            "Epoch ID: 1   Set ID: 13  Batch ID: 186 | Loss: 0.36344\n",
            "Epoch ID: 1   Set ID: 13  Batch ID: 187 | Loss: 0.36535\n",
            "Epoch ID: 1   Set ID: 13  Batch ID: 188 | Loss: 0.36298\n",
            "Epoch ID: 1   Set ID: 67  Batch ID: 189 | Loss: 0.35976\n",
            "Epoch ID: 1   Set ID: 67  Batch ID: 190 | Loss: 0.36315\n",
            "Epoch ID: 1   Set ID: 67  Batch ID: 191 | Loss: 0.35692\n",
            "Epoch ID: 1   Set ID: 67  Batch ID: 192 | Loss: 0.35665\n",
            "Epoch ID: 1   Set ID: 2  Batch ID: 193 | Loss: 0.35286\n",
            "Epoch ID: 1   Set ID: 2  Batch ID: 194 | Loss: 0.35695\n",
            "Epoch ID: 1   Set ID: 2  Batch ID: 195 | Loss: 0.35190\n",
            "Epoch ID: 1   Set ID: 2  Batch ID: 196 | Loss: 0.35634\n",
            "Epoch ID: 1   Set ID: 7  Batch ID: 197 | Loss: 0.36008\n",
            "Epoch ID: 1   Set ID: 7  Batch ID: 198 | Loss: 0.35619\n",
            "Epoch ID: 1   Set ID: 7  Batch ID: 199 | Loss: 0.34907\n",
            "Epoch ID: 1   Set ID: 7  Batch ID: 200 | Loss: 0.35516\n",
            "Epoch ID: 1   Set ID: 47  Batch ID: 201 | Loss: 0.35678\n",
            "Epoch ID: 1   Set ID: 47  Batch ID: 202 | Loss: 0.35753\n",
            "Epoch ID: 1   Set ID: 47  Batch ID: 203 | Loss: 0.35858\n",
            "Epoch ID: 1   Set ID: 47  Batch ID: 204 | Loss: 0.35189\n",
            "Epoch ID: 1   Set ID: 41  Batch ID: 205 | Loss: 0.34911\n",
            "Epoch ID: 1   Set ID: 41  Batch ID: 206 | Loss: 0.34849\n",
            "Epoch ID: 1   Set ID: 41  Batch ID: 207 | Loss: 0.34680\n",
            "Epoch ID: 1   Set ID: 41  Batch ID: 208 | Loss: 0.35348\n",
            "Epoch ID: 1   Set ID: 78  Batch ID: 209 | Loss: 0.34508\n",
            "Epoch ID: 1   Set ID: 78  Batch ID: 210 | Loss: 0.34796\n",
            "Epoch ID: 1   Set ID: 78  Batch ID: 211 | Loss: 0.35223\n",
            "Epoch ID: 1   Set ID: 78  Batch ID: 212 | Loss: 0.34553\n",
            "Epoch ID: 1   Set ID: 37  Batch ID: 213 | Loss: 0.35390\n",
            "Epoch ID: 1   Set ID: 37  Batch ID: 214 | Loss: 0.36229\n",
            "Epoch ID: 1   Set ID: 37  Batch ID: 215 | Loss: 0.34740\n",
            "Epoch ID: 1   Set ID: 37  Batch ID: 216 | Loss: 0.35452\n",
            "Epoch ID: 1   Set ID: 70  Batch ID: 217 | Loss: 0.35074\n",
            "Epoch ID: 1   Set ID: 70  Batch ID: 218 | Loss: 0.37043\n",
            "Epoch ID: 1   Set ID: 70  Batch ID: 219 | Loss: 0.36857\n",
            "Epoch ID: 1   Set ID: 70  Batch ID: 220 | Loss: 0.34807\n",
            "Epoch ID: 1   Set ID: 22  Batch ID: 221 | Loss: 0.37083\n",
            "Epoch ID: 1   Set ID: 22  Batch ID: 222 | Loss: 0.36925\n",
            "Epoch ID: 1   Set ID: 22  Batch ID: 223 | Loss: 0.35371\n",
            "Epoch ID: 1   Set ID: 22  Batch ID: 224 | Loss: 0.36723\n",
            "Epoch ID: 1   Set ID: 68  Batch ID: 225 | Loss: 0.37314\n",
            "Epoch ID: 1   Set ID: 68  Batch ID: 226 | Loss: 0.36893\n",
            "Epoch ID: 1   Set ID: 68  Batch ID: 227 | Loss: 0.35282\n",
            "Epoch ID: 1   Set ID: 68  Batch ID: 228 | Loss: 0.36109\n",
            "Epoch ID: 1   Set ID: 42  Batch ID: 229 | Loss: 0.36411\n",
            "Epoch ID: 1   Set ID: 42  Batch ID: 230 | Loss: 0.35498\n",
            "Epoch ID: 1   Set ID: 42  Batch ID: 231 | Loss: 0.35230\n",
            "Epoch ID: 1   Set ID: 42  Batch ID: 232 | Loss: 0.34574\n",
            "Epoch ID: 1   Set ID: 49  Batch ID: 233 | Loss: 0.34983\n",
            "Epoch ID: 1   Set ID: 49  Batch ID: 234 | Loss: 0.36566\n",
            "Epoch ID: 1   Set ID: 49  Batch ID: 235 | Loss: 0.34859\n",
            "Epoch ID: 1   Set ID: 49  Batch ID: 236 | Loss: 0.35932\n",
            "Epoch ID: 1   Set ID: 16  Batch ID: 237 | Loss: 0.34986\n",
            "Epoch ID: 1   Set ID: 16  Batch ID: 238 | Loss: 0.34538\n",
            "Epoch ID: 1   Set ID: 16  Batch ID: 239 | Loss: 0.34940\n",
            "Epoch ID: 1   Set ID: 16  Batch ID: 240 | Loss: 0.34308\n",
            "Epoch ID: 1   Set ID: 14  Batch ID: 241 | Loss: 0.34380\n",
            "Epoch ID: 1   Set ID: 14  Batch ID: 242 | Loss: 0.34543\n",
            "Epoch ID: 1   Set ID: 14  Batch ID: 243 | Loss: 0.34254\n",
            "Epoch ID: 1   Set ID: 14  Batch ID: 244 | Loss: 0.34719\n",
            "Epoch ID: 1   Set ID: 15  Batch ID: 245 | Loss: 0.34907\n",
            "Epoch ID: 1   Set ID: 15  Batch ID: 246 | Loss: 0.33562\n",
            "Epoch ID: 1   Set ID: 15  Batch ID: 247 | Loss: 0.33698\n",
            "Epoch ID: 1   Set ID: 15  Batch ID: 248 | Loss: 0.33958\n",
            "Epoch ID: 1   Set ID: 20  Batch ID: 249 | Loss: 0.33663\n",
            "Epoch ID: 1   Set ID: 20  Batch ID: 250 | Loss: 0.33249\n",
            "Epoch ID: 1   Set ID: 20  Batch ID: 251 | Loss: 0.33101\n",
            "Epoch ID: 1   Set ID: 20  Batch ID: 252 | Loss: 0.32784\n",
            "Epoch ID: 1   Set ID: 3  Batch ID: 253 | Loss: 0.33246\n",
            "Epoch ID: 1   Set ID: 3  Batch ID: 254 | Loss: 0.32536\n",
            "Epoch ID: 1   Set ID: 3  Batch ID: 255 | Loss: 0.32181\n",
            "Epoch ID: 1   Set ID: 3  Batch ID: 256 | Loss: 0.32222\n",
            "Epoch ID: 2   Set ID: 34  Batch ID: 1 | Loss: 0.32907\n",
            "Epoch ID: 2   Set ID: 34  Batch ID: 2 | Loss: 0.32826\n",
            "Epoch ID: 2   Set ID: 34  Batch ID: 3 | Loss: 0.31849\n",
            "Epoch ID: 2   Set ID: 34  Batch ID: 4 | Loss: 0.32768\n",
            "Epoch ID: 2   Set ID: 79  Batch ID: 5 | Loss: 0.32011\n",
            "Epoch ID: 2   Set ID: 79  Batch ID: 6 | Loss: 0.31868\n",
            "Epoch ID: 2   Set ID: 79  Batch ID: 7 | Loss: 0.31148\n",
            "Epoch ID: 2   Set ID: 79  Batch ID: 8 | Loss: 0.31605\n",
            "Epoch ID: 2   Set ID: 55  Batch ID: 9 | Loss: 0.31240\n",
            "Epoch ID: 2   Set ID: 55  Batch ID: 10 | Loss: 0.31210\n",
            "Epoch ID: 2   Set ID: 55  Batch ID: 11 | Loss: 0.30783\n",
            "Epoch ID: 2   Set ID: 55  Batch ID: 12 | Loss: 0.30157\n",
            "Epoch ID: 2   Set ID: 60  Batch ID: 13 | Loss: 0.30457\n",
            "Epoch ID: 2   Set ID: 60  Batch ID: 14 | Loss: 0.30191\n",
            "Epoch ID: 2   Set ID: 60  Batch ID: 15 | Loss: 0.30087\n",
            "Epoch ID: 2   Set ID: 60  Batch ID: 16 | Loss: 0.29867\n",
            "Epoch ID: 2   Set ID: 27  Batch ID: 17 | Loss: 0.29529\n",
            "Epoch ID: 2   Set ID: 27  Batch ID: 18 | Loss: 0.29139\n",
            "Epoch ID: 2   Set ID: 27  Batch ID: 19 | Loss: 0.28443\n",
            "Epoch ID: 2   Set ID: 27  Batch ID: 20 | Loss: 0.27368\n",
            "Epoch ID: 2   Set ID: 51  Batch ID: 21 | Loss: 0.26388\n",
            "Epoch ID: 2   Set ID: 51  Batch ID: 22 | Loss: 0.26671\n",
            "Epoch ID: 2   Set ID: 51  Batch ID: 23 | Loss: 0.25111\n",
            "Epoch ID: 2   Set ID: 51  Batch ID: 24 | Loss: 0.24560\n",
            "Epoch ID: 2   Set ID: 62  Batch ID: 25 | Loss: 0.23753\n",
            "Epoch ID: 2   Set ID: 62  Batch ID: 26 | Loss: 0.23195\n",
            "Epoch ID: 2   Set ID: 62  Batch ID: 27 | Loss: 0.20387\n",
            "Epoch ID: 2   Set ID: 62  Batch ID: 28 | Loss: 0.10940\n",
            "Epoch ID: 2   Set ID: 75  Batch ID: 29 | Loss: 0.10916\n",
            "Epoch ID: 2   Set ID: 75  Batch ID: 30 | Loss: 0.10923\n",
            "Epoch ID: 2   Set ID: 75  Batch ID: 31 | Loss: 0.10807\n",
            "Epoch ID: 2   Set ID: 75  Batch ID: 32 | Loss: 0.10830\n",
            "Epoch ID: 2   Set ID: 21  Batch ID: 33 | Loss: 0.10741\n",
            "Epoch ID: 2   Set ID: 21  Batch ID: 34 | Loss: 0.10792\n",
            "Epoch ID: 2   Set ID: 21  Batch ID: 35 | Loss: 0.10881\n",
            "Epoch ID: 2   Set ID: 21  Batch ID: 36 | Loss: 0.12807\n",
            "Epoch ID: 2   Set ID: 4  Batch ID: 37 | Loss: 0.10985\n",
            "Epoch ID: 2   Set ID: 4  Batch ID: 38 | Loss: 0.10506\n",
            "Epoch ID: 2   Set ID: 4  Batch ID: 39 | Loss: 0.10601\n",
            "Epoch ID: 2   Set ID: 4  Batch ID: 40 | Loss: 0.11021\n",
            "Epoch ID: 2   Set ID: 76  Batch ID: 41 | Loss: 0.12382\n",
            "Epoch ID: 2   Set ID: 76  Batch ID: 42 | Loss: 0.10696\n",
            "Epoch ID: 2   Set ID: 76  Batch ID: 43 | Loss: 0.12187\n",
            "Epoch ID: 2   Set ID: 76  Batch ID: 44 | Loss: 0.10997\n",
            "Epoch ID: 2   Set ID: 43  Batch ID: 45 | Loss: 0.10591\n",
            "Epoch ID: 2   Set ID: 43  Batch ID: 46 | Loss: 0.10839\n",
            "Epoch ID: 2   Set ID: 43  Batch ID: 47 | Loss: 0.11070\n",
            "Epoch ID: 2   Set ID: 43  Batch ID: 48 | Loss: 0.10875\n",
            "Epoch ID: 2   Set ID: 17  Batch ID: 49 | Loss: 0.11082\n",
            "Epoch ID: 2   Set ID: 17  Batch ID: 50 | Loss: 0.10706\n",
            "Epoch ID: 2   Set ID: 17  Batch ID: 51 | Loss: 0.12819\n",
            "Epoch ID: 2   Set ID: 17  Batch ID: 52 | Loss: 0.10852\n",
            "Epoch ID: 2   Set ID: 36  Batch ID: 53 | Loss: 0.10767\n",
            "Epoch ID: 2   Set ID: 36  Batch ID: 54 | Loss: 0.10692\n",
            "Epoch ID: 2   Set ID: 36  Batch ID: 55 | Loss: 0.12811\n",
            "Epoch ID: 2   Set ID: 36  Batch ID: 56 | Loss: 0.10511\n",
            "Epoch ID: 2   Set ID: 59  Batch ID: 57 | Loss: 0.10811\n",
            "Epoch ID: 2   Set ID: 59  Batch ID: 58 | Loss: 0.10493\n",
            "Epoch ID: 2   Set ID: 59  Batch ID: 59 | Loss: 0.12889\n",
            "Epoch ID: 2   Set ID: 59  Batch ID: 60 | Loss: 0.10982\n",
            "Epoch ID: 2   Set ID: 9  Batch ID: 61 | Loss: 0.10751\n",
            "Epoch ID: 2   Set ID: 9  Batch ID: 62 | Loss: 0.10539\n",
            "Epoch ID: 2   Set ID: 9  Batch ID: 63 | Loss: 0.10515\n",
            "Epoch ID: 2   Set ID: 9  Batch ID: 64 | Loss: 0.10316\n",
            "Epoch ID: 2   Set ID: 69  Batch ID: 65 | Loss: 0.10474\n",
            "Epoch ID: 2   Set ID: 69  Batch ID: 66 | Loss: 0.11218\n",
            "Epoch ID: 2   Set ID: 69  Batch ID: 67 | Loss: 0.10433\n",
            "Epoch ID: 2   Set ID: 69  Batch ID: 68 | Loss: 0.11240\n",
            "Epoch ID: 2   Set ID: 57  Batch ID: 69 | Loss: 0.10604\n",
            "Epoch ID: 2   Set ID: 57  Batch ID: 70 | Loss: 0.10155\n",
            "Epoch ID: 2   Set ID: 57  Batch ID: 71 | Loss: 0.10485\n",
            "Epoch ID: 2   Set ID: 57  Batch ID: 72 | Loss: 0.10370\n",
            "Epoch ID: 2   Set ID: 56  Batch ID: 73 | Loss: 0.10100\n",
            "Epoch ID: 2   Set ID: 56  Batch ID: 74 | Loss: 0.10373\n",
            "Epoch ID: 2   Set ID: 56  Batch ID: 75 | Loss: 0.10173\n",
            "Epoch ID: 2   Set ID: 56  Batch ID: 76 | Loss: 0.10278\n",
            "Epoch ID: 2   Set ID: 26  Batch ID: 77 | Loss: 0.10131\n",
            "Epoch ID: 2   Set ID: 26  Batch ID: 78 | Loss: 0.10203\n",
            "Epoch ID: 2   Set ID: 26  Batch ID: 79 | Loss: 0.10355\n",
            "Epoch ID: 2   Set ID: 26  Batch ID: 80 | Loss: 0.10360\n",
            "Epoch ID: 2   Set ID: 35  Batch ID: 81 | Loss: 0.10103\n",
            "Epoch ID: 2   Set ID: 35  Batch ID: 82 | Loss: 0.10370\n",
            "Epoch ID: 2   Set ID: 35  Batch ID: 83 | Loss: 0.10369\n",
            "Epoch ID: 2   Set ID: 35  Batch ID: 84 | Loss: 0.10435\n",
            "Epoch ID: 2   Set ID: 44  Batch ID: 85 | Loss: 0.10076\n",
            "Epoch ID: 2   Set ID: 44  Batch ID: 86 | Loss: 0.10197\n",
            "Epoch ID: 2   Set ID: 44  Batch ID: 87 | Loss: 0.10262\n",
            "Epoch ID: 2   Set ID: 44  Batch ID: 88 | Loss: 0.10355\n",
            "Epoch ID: 2   Set ID: 40  Batch ID: 89 | Loss: 0.10338\n",
            "Epoch ID: 2   Set ID: 40  Batch ID: 90 | Loss: 0.10092\n",
            "Epoch ID: 2   Set ID: 40  Batch ID: 91 | Loss: 0.10020\n",
            "Epoch ID: 2   Set ID: 40  Batch ID: 92 | Loss: 0.10221\n",
            "Epoch ID: 2   Set ID: 8  Batch ID: 93 | Loss: 0.10167\n",
            "Epoch ID: 2   Set ID: 8  Batch ID: 94 | Loss: 0.10046\n",
            "Epoch ID: 2   Set ID: 8  Batch ID: 95 | Loss: 0.10243\n",
            "Epoch ID: 2   Set ID: 8  Batch ID: 96 | Loss: 0.10269\n",
            "Epoch ID: 2   Set ID: 31  Batch ID: 97 | Loss: 0.10115\n",
            "Epoch ID: 2   Set ID: 31  Batch ID: 98 | Loss: 0.09984\n",
            "Epoch ID: 2   Set ID: 31  Batch ID: 99 | Loss: 0.10329\n",
            "Epoch ID: 2   Set ID: 31  Batch ID: 100 | Loss: 0.10175\n",
            "Epoch ID: 2   Set ID: 32  Batch ID: 101 | Loss: 0.10240\n",
            "Epoch ID: 2   Set ID: 32  Batch ID: 102 | Loss: 0.10243\n",
            "Epoch ID: 2   Set ID: 32  Batch ID: 103 | Loss: 0.10154\n",
            "Epoch ID: 2   Set ID: 32  Batch ID: 104 | Loss: 0.10220\n",
            "Epoch ID: 2   Set ID: 11  Batch ID: 105 | Loss: 0.10235\n",
            "Epoch ID: 2   Set ID: 11  Batch ID: 106 | Loss: 0.09969\n",
            "Epoch ID: 2   Set ID: 11  Batch ID: 107 | Loss: 0.10166\n",
            "Epoch ID: 2   Set ID: 11  Batch ID: 108 | Loss: 0.10086\n",
            "Epoch ID: 2   Set ID: 45  Batch ID: 109 | Loss: 0.10182\n",
            "Epoch ID: 2   Set ID: 45  Batch ID: 110 | Loss: 0.09810\n",
            "Epoch ID: 2   Set ID: 45  Batch ID: 111 | Loss: 0.10129\n",
            "Epoch ID: 2   Set ID: 45  Batch ID: 112 | Loss: 0.09933\n",
            "Epoch ID: 2   Set ID: 64  Batch ID: 113 | Loss: 0.10042\n",
            "Epoch ID: 2   Set ID: 64  Batch ID: 114 | Loss: 0.09962\n",
            "Epoch ID: 2   Set ID: 64  Batch ID: 115 | Loss: 0.10119\n",
            "Epoch ID: 2   Set ID: 64  Batch ID: 116 | Loss: 0.09950\n",
            "Epoch ID: 2   Set ID: 29  Batch ID: 117 | Loss: 0.10139\n",
            "Epoch ID: 2   Set ID: 29  Batch ID: 118 | Loss: 0.10229\n",
            "Epoch ID: 2   Set ID: 29  Batch ID: 119 | Loss: 0.10267\n",
            "Epoch ID: 2   Set ID: 29  Batch ID: 120 | Loss: 0.10269\n",
            "Epoch ID: 2   Set ID: 5  Batch ID: 121 | Loss: 0.09905\n",
            "Epoch ID: 2   Set ID: 5  Batch ID: 122 | Loss: 0.10172\n",
            "Epoch ID: 2   Set ID: 5  Batch ID: 123 | Loss: 0.09916\n",
            "Epoch ID: 2   Set ID: 5  Batch ID: 124 | Loss: 0.10348\n",
            "Epoch ID: 2   Set ID: 53  Batch ID: 125 | Loss: 0.09872\n",
            "Epoch ID: 2   Set ID: 53  Batch ID: 126 | Loss: 0.09983\n",
            "Epoch ID: 2   Set ID: 53  Batch ID: 127 | Loss: 0.09960\n",
            "Epoch ID: 2   Set ID: 53  Batch ID: 128 | Loss: 0.10177\n",
            "Epoch ID: 2   Set ID: 12  Batch ID: 129 | Loss: 0.10297\n",
            "Epoch ID: 2   Set ID: 12  Batch ID: 130 | Loss: 0.10019\n",
            "Epoch ID: 2   Set ID: 12  Batch ID: 131 | Loss: 0.09960\n",
            "Epoch ID: 2   Set ID: 12  Batch ID: 132 | Loss: 0.10079\n",
            "Epoch ID: 2   Set ID: 39  Batch ID: 133 | Loss: 0.10042\n",
            "Epoch ID: 2   Set ID: 39  Batch ID: 134 | Loss: 0.10229\n",
            "Epoch ID: 2   Set ID: 39  Batch ID: 135 | Loss: 0.10220\n",
            "Epoch ID: 2   Set ID: 39  Batch ID: 136 | Loss: 0.10536\n",
            "Epoch ID: 2   Set ID: 50  Batch ID: 137 | Loss: 0.10061\n",
            "Epoch ID: 2   Set ID: 50  Batch ID: 138 | Loss: 0.10476\n",
            "Epoch ID: 2   Set ID: 50  Batch ID: 139 | Loss: 0.10225\n",
            "Epoch ID: 2   Set ID: 50  Batch ID: 140 | Loss: 0.09929\n",
            "Epoch ID: 2   Set ID: 24  Batch ID: 141 | Loss: 0.10324\n",
            "Epoch ID: 2   Set ID: 24  Batch ID: 142 | Loss: 0.10114\n",
            "Epoch ID: 2   Set ID: 24  Batch ID: 143 | Loss: 0.09869\n",
            "Epoch ID: 2   Set ID: 24  Batch ID: 144 | Loss: 0.09959\n",
            "Epoch ID: 2   Set ID: 25  Batch ID: 145 | Loss: 0.09929\n",
            "Epoch ID: 2   Set ID: 25  Batch ID: 146 | Loss: 0.09989\n",
            "Epoch ID: 2   Set ID: 25  Batch ID: 147 | Loss: 0.09781\n",
            "Epoch ID: 2   Set ID: 25  Batch ID: 148 | Loss: 0.09999\n",
            "Epoch ID: 2   Set ID: 71  Batch ID: 149 | Loss: 0.09949\n",
            "Epoch ID: 2   Set ID: 71  Batch ID: 150 | Loss: 0.10553\n",
            "Epoch ID: 2   Set ID: 71  Batch ID: 151 | Loss: 0.10114\n",
            "Epoch ID: 2   Set ID: 71  Batch ID: 152 | Loss: 0.10025\n",
            "Epoch ID: 2   Set ID: 1  Batch ID: 153 | Loss: 0.10166\n",
            "Epoch ID: 2   Set ID: 1  Batch ID: 154 | Loss: 0.09778\n",
            "Epoch ID: 2   Set ID: 1  Batch ID: 155 | Loss: 0.09863\n",
            "Epoch ID: 2   Set ID: 1  Batch ID: 156 | Loss: 0.10126\n",
            "Epoch ID: 2   Set ID: 66  Batch ID: 157 | Loss: 0.09927\n",
            "Epoch ID: 2   Set ID: 66  Batch ID: 158 | Loss: 0.09966\n",
            "Epoch ID: 2   Set ID: 66  Batch ID: 159 | Loss: 0.10158\n",
            "Epoch ID: 2   Set ID: 66  Batch ID: 160 | Loss: 0.10016\n",
            "Epoch ID: 2   Set ID: 61  Batch ID: 161 | Loss: 0.10024\n",
            "Epoch ID: 2   Set ID: 61  Batch ID: 162 | Loss: 0.10110\n",
            "Epoch ID: 2   Set ID: 61  Batch ID: 163 | Loss: 0.10009\n",
            "Epoch ID: 2   Set ID: 61  Batch ID: 164 | Loss: 0.09969\n",
            "Epoch ID: 2   Set ID: 28  Batch ID: 165 | Loss: 0.09890\n",
            "Epoch ID: 2   Set ID: 28  Batch ID: 166 | Loss: 0.09690\n",
            "Epoch ID: 2   Set ID: 28  Batch ID: 167 | Loss: 0.10079\n",
            "Epoch ID: 2   Set ID: 28  Batch ID: 168 | Loss: 0.09891\n",
            "Epoch ID: 2   Set ID: 73  Batch ID: 169 | Loss: 0.09772\n",
            "Epoch ID: 2   Set ID: 73  Batch ID: 170 | Loss: 0.10002\n",
            "Epoch ID: 2   Set ID: 73  Batch ID: 171 | Loss: 0.09892\n",
            "Epoch ID: 2   Set ID: 73  Batch ID: 172 | Loss: 0.09767\n",
            "Epoch ID: 2   Set ID: 23  Batch ID: 173 | Loss: 0.09876\n",
            "Epoch ID: 2   Set ID: 23  Batch ID: 174 | Loss: 0.09824\n",
            "Epoch ID: 2   Set ID: 23  Batch ID: 175 | Loss: 0.10004\n",
            "Epoch ID: 2   Set ID: 23  Batch ID: 176 | Loss: 0.09990\n",
            "Epoch ID: 2   Set ID: 10  Batch ID: 177 | Loss: 0.09909\n",
            "Epoch ID: 2   Set ID: 10  Batch ID: 178 | Loss: 0.10166\n",
            "Epoch ID: 2   Set ID: 10  Batch ID: 179 | Loss: 0.09727\n",
            "Epoch ID: 2   Set ID: 10  Batch ID: 180 | Loss: 0.09870\n",
            "Epoch ID: 2   Set ID: 30  Batch ID: 181 | Loss: 0.09912\n",
            "Epoch ID: 2   Set ID: 30  Batch ID: 182 | Loss: 0.10014\n",
            "Epoch ID: 2   Set ID: 30  Batch ID: 183 | Loss: 0.09870\n",
            "Epoch ID: 2   Set ID: 30  Batch ID: 184 | Loss: 0.09935\n",
            "Epoch ID: 2   Set ID: 13  Batch ID: 185 | Loss: 0.10084\n",
            "Epoch ID: 2   Set ID: 13  Batch ID: 186 | Loss: 0.09890\n",
            "Epoch ID: 2   Set ID: 13  Batch ID: 187 | Loss: 0.09790\n",
            "Epoch ID: 2   Set ID: 13  Batch ID: 188 | Loss: 0.09914\n",
            "Epoch ID: 2   Set ID: 67  Batch ID: 189 | Loss: 0.09805\n",
            "Epoch ID: 2   Set ID: 67  Batch ID: 190 | Loss: 0.09751\n",
            "Epoch ID: 2   Set ID: 67  Batch ID: 191 | Loss: 0.09991\n",
            "Epoch ID: 2   Set ID: 67  Batch ID: 192 | Loss: 0.09911\n",
            "Epoch ID: 2   Set ID: 2  Batch ID: 193 | Loss: 0.09789\n",
            "Epoch ID: 2   Set ID: 2  Batch ID: 194 | Loss: 0.09571\n",
            "Epoch ID: 2   Set ID: 2  Batch ID: 195 | Loss: 0.09907\n",
            "Epoch ID: 2   Set ID: 2  Batch ID: 196 | Loss: 0.09976\n",
            "Epoch ID: 2   Set ID: 7  Batch ID: 197 | Loss: 0.10069\n",
            "Epoch ID: 2   Set ID: 7  Batch ID: 198 | Loss: 0.09857\n",
            "Epoch ID: 2   Set ID: 7  Batch ID: 199 | Loss: 0.09803\n",
            "Epoch ID: 2   Set ID: 7  Batch ID: 200 | Loss: 0.09773\n",
            "Epoch ID: 2   Set ID: 47  Batch ID: 201 | Loss: 0.09834\n",
            "Epoch ID: 2   Set ID: 47  Batch ID: 202 | Loss: 0.09724\n",
            "Epoch ID: 2   Set ID: 47  Batch ID: 203 | Loss: 0.09735\n",
            "Epoch ID: 2   Set ID: 47  Batch ID: 204 | Loss: 0.09879\n",
            "Epoch ID: 2   Set ID: 41  Batch ID: 205 | Loss: 0.09683\n",
            "Epoch ID: 2   Set ID: 41  Batch ID: 206 | Loss: 0.09888\n",
            "Epoch ID: 2   Set ID: 41  Batch ID: 207 | Loss: 0.09790\n",
            "Epoch ID: 2   Set ID: 41  Batch ID: 208 | Loss: 0.09765\n",
            "Epoch ID: 2   Set ID: 78  Batch ID: 209 | Loss: 0.09858\n",
            "Epoch ID: 2   Set ID: 78  Batch ID: 210 | Loss: 0.09943\n",
            "Epoch ID: 2   Set ID: 78  Batch ID: 211 | Loss: 0.09755\n",
            "Epoch ID: 2   Set ID: 78  Batch ID: 212 | Loss: 0.09757\n",
            "Epoch ID: 2   Set ID: 37  Batch ID: 213 | Loss: 0.09688\n",
            "Epoch ID: 2   Set ID: 37  Batch ID: 214 | Loss: 0.09715\n",
            "Epoch ID: 2   Set ID: 37  Batch ID: 215 | Loss: 0.09511\n",
            "Epoch ID: 2   Set ID: 37  Batch ID: 216 | Loss: 0.09881\n",
            "Epoch ID: 2   Set ID: 70  Batch ID: 217 | Loss: 0.09834\n",
            "Epoch ID: 2   Set ID: 70  Batch ID: 218 | Loss: 0.09747\n",
            "Epoch ID: 2   Set ID: 70  Batch ID: 219 | Loss: 0.09704\n",
            "Epoch ID: 2   Set ID: 70  Batch ID: 220 | Loss: 0.09686\n",
            "Epoch ID: 2   Set ID: 22  Batch ID: 221 | Loss: 0.09963\n",
            "Epoch ID: 2   Set ID: 22  Batch ID: 222 | Loss: 0.09827\n",
            "Epoch ID: 2   Set ID: 22  Batch ID: 223 | Loss: 0.09699\n",
            "Epoch ID: 2   Set ID: 22  Batch ID: 224 | Loss: 0.09743\n",
            "Epoch ID: 2   Set ID: 68  Batch ID: 225 | Loss: 0.09642\n",
            "Epoch ID: 2   Set ID: 68  Batch ID: 226 | Loss: 0.09516\n",
            "Epoch ID: 2   Set ID: 68  Batch ID: 227 | Loss: 0.09579\n",
            "Epoch ID: 2   Set ID: 68  Batch ID: 228 | Loss: 0.09984\n",
            "Epoch ID: 2   Set ID: 42  Batch ID: 229 | Loss: 0.09678\n",
            "Epoch ID: 2   Set ID: 42  Batch ID: 230 | Loss: 0.09507\n",
            "Epoch ID: 2   Set ID: 42  Batch ID: 231 | Loss: 0.09946\n",
            "Epoch ID: 2   Set ID: 42  Batch ID: 232 | Loss: 0.09564\n",
            "Epoch ID: 2   Set ID: 49  Batch ID: 233 | Loss: 0.09732\n",
            "Epoch ID: 2   Set ID: 49  Batch ID: 234 | Loss: 0.09643\n",
            "Epoch ID: 2   Set ID: 49  Batch ID: 235 | Loss: 0.09816\n",
            "Epoch ID: 2   Set ID: 49  Batch ID: 236 | Loss: 0.09649\n",
            "Epoch ID: 2   Set ID: 16  Batch ID: 237 | Loss: 0.09743\n",
            "Epoch ID: 2   Set ID: 16  Batch ID: 238 | Loss: 0.09687\n",
            "Epoch ID: 2   Set ID: 16  Batch ID: 239 | Loss: 0.09561\n",
            "Epoch ID: 2   Set ID: 16  Batch ID: 240 | Loss: 0.09657\n",
            "Epoch ID: 2   Set ID: 14  Batch ID: 241 | Loss: 0.09788\n",
            "Epoch ID: 2   Set ID: 14  Batch ID: 242 | Loss: 0.09653\n",
            "Epoch ID: 2   Set ID: 14  Batch ID: 243 | Loss: 0.09677\n",
            "Epoch ID: 2   Set ID: 14  Batch ID: 244 | Loss: 0.09595\n",
            "Epoch ID: 2   Set ID: 15  Batch ID: 245 | Loss: 0.09572\n",
            "Epoch ID: 2   Set ID: 15  Batch ID: 246 | Loss: 0.09625\n",
            "Epoch ID: 2   Set ID: 15  Batch ID: 247 | Loss: 0.09590\n",
            "Epoch ID: 2   Set ID: 15  Batch ID: 248 | Loss: 0.09661\n",
            "Epoch ID: 2   Set ID: 20  Batch ID: 249 | Loss: 0.09660\n",
            "Epoch ID: 2   Set ID: 20  Batch ID: 250 | Loss: 0.09690\n",
            "Epoch ID: 2   Set ID: 20  Batch ID: 251 | Loss: 0.09735\n",
            "Epoch ID: 2   Set ID: 20  Batch ID: 252 | Loss: 0.09703\n",
            "Epoch ID: 2   Set ID: 3  Batch ID: 253 | Loss: 0.09700\n",
            "Epoch ID: 2   Set ID: 3  Batch ID: 254 | Loss: 0.09481\n",
            "Epoch ID: 2   Set ID: 3  Batch ID: 255 | Loss: 0.09819\n",
            "Epoch ID: 2   Set ID: 3  Batch ID: 256 | Loss: 0.09546\n",
            "Epoch ID: 3   Set ID: 34  Batch ID: 1 | Loss: 0.09501\n",
            "Epoch ID: 3   Set ID: 34  Batch ID: 2 | Loss: 0.09623\n",
            "Epoch ID: 3   Set ID: 34  Batch ID: 3 | Loss: 0.09884\n",
            "Epoch ID: 3   Set ID: 34  Batch ID: 4 | Loss: 0.09839\n",
            "Epoch ID: 3   Set ID: 79  Batch ID: 5 | Loss: 0.09714\n",
            "Epoch ID: 3   Set ID: 79  Batch ID: 6 | Loss: 0.09642\n",
            "Epoch ID: 3   Set ID: 79  Batch ID: 7 | Loss: 0.09806\n",
            "Epoch ID: 3   Set ID: 79  Batch ID: 8 | Loss: 0.09498\n",
            "Epoch ID: 3   Set ID: 55  Batch ID: 9 | Loss: 0.09325\n",
            "Epoch ID: 3   Set ID: 55  Batch ID: 10 | Loss: 0.09672\n",
            "Epoch ID: 3   Set ID: 55  Batch ID: 11 | Loss: 0.09586\n",
            "Epoch ID: 3   Set ID: 55  Batch ID: 12 | Loss: 0.09891\n",
            "Epoch ID: 3   Set ID: 60  Batch ID: 13 | Loss: 0.09532\n",
            "Epoch ID: 3   Set ID: 60  Batch ID: 14 | Loss: 0.09164\n",
            "Epoch ID: 3   Set ID: 60  Batch ID: 15 | Loss: 0.09646\n",
            "Epoch ID: 3   Set ID: 60  Batch ID: 16 | Loss: 0.09752\n",
            "Epoch ID: 3   Set ID: 27  Batch ID: 17 | Loss: 0.09277\n",
            "Epoch ID: 3   Set ID: 27  Batch ID: 18 | Loss: 0.09515\n",
            "Epoch ID: 3   Set ID: 27  Batch ID: 19 | Loss: 0.09801\n",
            "Epoch ID: 3   Set ID: 27  Batch ID: 20 | Loss: 0.09698\n",
            "Epoch ID: 3   Set ID: 51  Batch ID: 21 | Loss: 0.09370\n",
            "Epoch ID: 3   Set ID: 51  Batch ID: 22 | Loss: 0.09714\n",
            "Epoch ID: 3   Set ID: 51  Batch ID: 23 | Loss: 0.09473\n",
            "Epoch ID: 3   Set ID: 51  Batch ID: 24 | Loss: 0.09833\n",
            "Epoch ID: 3   Set ID: 62  Batch ID: 25 | Loss: 0.09676\n",
            "Epoch ID: 3   Set ID: 62  Batch ID: 26 | Loss: 0.09607\n",
            "Epoch ID: 3   Set ID: 62  Batch ID: 27 | Loss: 0.09362\n",
            "Epoch ID: 3   Set ID: 62  Batch ID: 28 | Loss: 0.09751\n",
            "Epoch ID: 3   Set ID: 75  Batch ID: 29 | Loss: 0.09543\n",
            "Epoch ID: 3   Set ID: 75  Batch ID: 30 | Loss: 0.09544\n",
            "Epoch ID: 3   Set ID: 75  Batch ID: 31 | Loss: 0.09688\n",
            "Epoch ID: 3   Set ID: 75  Batch ID: 32 | Loss: 0.09643\n",
            "Epoch ID: 3   Set ID: 21  Batch ID: 33 | Loss: 0.09623\n",
            "Epoch ID: 3   Set ID: 21  Batch ID: 34 | Loss: 0.09641\n",
            "Epoch ID: 3   Set ID: 21  Batch ID: 35 | Loss: 0.09496\n",
            "Epoch ID: 3   Set ID: 21  Batch ID: 36 | Loss: 0.09591\n",
            "Epoch ID: 3   Set ID: 4  Batch ID: 37 | Loss: 0.09699\n",
            "Epoch ID: 3   Set ID: 4  Batch ID: 38 | Loss: 0.09437\n",
            "Epoch ID: 3   Set ID: 4  Batch ID: 39 | Loss: 0.09744\n",
            "Epoch ID: 3   Set ID: 4  Batch ID: 40 | Loss: 0.09517\n",
            "Epoch ID: 3   Set ID: 76  Batch ID: 41 | Loss: 0.09666\n",
            "Epoch ID: 3   Set ID: 76  Batch ID: 42 | Loss: 0.09425\n",
            "Epoch ID: 3   Set ID: 76  Batch ID: 43 | Loss: 0.09492\n",
            "Epoch ID: 3   Set ID: 76  Batch ID: 44 | Loss: 0.09523\n",
            "Epoch ID: 3   Set ID: 43  Batch ID: 45 | Loss: 0.09558\n",
            "Epoch ID: 3   Set ID: 43  Batch ID: 46 | Loss: 0.09704\n",
            "Epoch ID: 3   Set ID: 43  Batch ID: 47 | Loss: 0.09743\n",
            "Epoch ID: 3   Set ID: 43  Batch ID: 48 | Loss: 0.09545\n",
            "Epoch ID: 3   Set ID: 17  Batch ID: 49 | Loss: 0.09634\n",
            "Epoch ID: 3   Set ID: 17  Batch ID: 50 | Loss: 0.09657\n",
            "Epoch ID: 3   Set ID: 17  Batch ID: 51 | Loss: 0.09660\n",
            "Epoch ID: 3   Set ID: 17  Batch ID: 52 | Loss: 0.09440\n",
            "Epoch ID: 3   Set ID: 36  Batch ID: 53 | Loss: 0.09625\n",
            "Epoch ID: 3   Set ID: 36  Batch ID: 54 | Loss: 0.09432\n",
            "Epoch ID: 3   Set ID: 36  Batch ID: 55 | Loss: 0.09579\n",
            "Epoch ID: 3   Set ID: 36  Batch ID: 56 | Loss: 0.09646\n",
            "Epoch ID: 3   Set ID: 59  Batch ID: 57 | Loss: 0.09619\n",
            "Epoch ID: 3   Set ID: 59  Batch ID: 58 | Loss: 0.09667\n",
            "Epoch ID: 3   Set ID: 59  Batch ID: 59 | Loss: 0.09755\n",
            "Epoch ID: 3   Set ID: 59  Batch ID: 60 | Loss: 0.09471\n",
            "Epoch ID: 3   Set ID: 9  Batch ID: 61 | Loss: 0.09469\n",
            "Epoch ID: 3   Set ID: 9  Batch ID: 62 | Loss: 0.09661\n",
            "Epoch ID: 3   Set ID: 9  Batch ID: 63 | Loss: 0.09558\n",
            "Epoch ID: 3   Set ID: 9  Batch ID: 64 | Loss: 0.09376\n",
            "Epoch ID: 3   Set ID: 69  Batch ID: 65 | Loss: 0.09641\n",
            "Epoch ID: 3   Set ID: 69  Batch ID: 66 | Loss: 0.09592\n",
            "Epoch ID: 3   Set ID: 69  Batch ID: 67 | Loss: 0.09608\n",
            "Epoch ID: 3   Set ID: 69  Batch ID: 68 | Loss: 0.09391\n",
            "Epoch ID: 3   Set ID: 57  Batch ID: 69 | Loss: 0.09662\n",
            "Epoch ID: 3   Set ID: 57  Batch ID: 70 | Loss: 0.09735\n",
            "Epoch ID: 3   Set ID: 57  Batch ID: 71 | Loss: 0.09714\n",
            "Epoch ID: 3   Set ID: 57  Batch ID: 72 | Loss: 0.09709\n",
            "Epoch ID: 3   Set ID: 56  Batch ID: 73 | Loss: 0.09600\n",
            "Epoch ID: 3   Set ID: 56  Batch ID: 74 | Loss: 0.09496\n",
            "Epoch ID: 3   Set ID: 56  Batch ID: 75 | Loss: 0.09290\n",
            "Epoch ID: 3   Set ID: 56  Batch ID: 76 | Loss: 0.09461\n",
            "Epoch ID: 3   Set ID: 26  Batch ID: 77 | Loss: 0.09360\n",
            "Epoch ID: 3   Set ID: 26  Batch ID: 78 | Loss: 0.09654\n",
            "Epoch ID: 3   Set ID: 26  Batch ID: 79 | Loss: 0.09480\n",
            "Epoch ID: 3   Set ID: 26  Batch ID: 80 | Loss: 0.09699\n",
            "Epoch ID: 3   Set ID: 35  Batch ID: 81 | Loss: 0.09731\n",
            "Epoch ID: 3   Set ID: 35  Batch ID: 82 | Loss: 0.09620\n",
            "Epoch ID: 3   Set ID: 35  Batch ID: 83 | Loss: 0.09571\n",
            "Epoch ID: 3   Set ID: 35  Batch ID: 84 | Loss: 0.09505\n",
            "Epoch ID: 3   Set ID: 44  Batch ID: 85 | Loss: 0.09499\n",
            "Epoch ID: 3   Set ID: 44  Batch ID: 86 | Loss: 0.09670\n",
            "Epoch ID: 3   Set ID: 44  Batch ID: 87 | Loss: 0.09838\n",
            "Epoch ID: 3   Set ID: 44  Batch ID: 88 | Loss: 0.09768\n",
            "Epoch ID: 3   Set ID: 40  Batch ID: 89 | Loss: 0.09527\n",
            "Epoch ID: 3   Set ID: 40  Batch ID: 90 | Loss: 0.09589\n",
            "Epoch ID: 3   Set ID: 40  Batch ID: 91 | Loss: 0.09478\n",
            "Epoch ID: 3   Set ID: 40  Batch ID: 92 | Loss: 0.09645\n",
            "Epoch ID: 3   Set ID: 8  Batch ID: 93 | Loss: 0.09584\n",
            "Epoch ID: 3   Set ID: 8  Batch ID: 94 | Loss: 0.09669\n",
            "Epoch ID: 3   Set ID: 8  Batch ID: 95 | Loss: 0.09418\n",
            "Epoch ID: 3   Set ID: 8  Batch ID: 96 | Loss: 0.09564\n",
            "Epoch ID: 3   Set ID: 31  Batch ID: 97 | Loss: 0.09623\n",
            "Epoch ID: 3   Set ID: 31  Batch ID: 98 | Loss: 0.09291\n",
            "Epoch ID: 3   Set ID: 31  Batch ID: 99 | Loss: 0.09776\n",
            "Epoch ID: 3   Set ID: 31  Batch ID: 100 | Loss: 0.09492\n",
            "Epoch ID: 3   Set ID: 32  Batch ID: 101 | Loss: 0.09671\n",
            "Epoch ID: 3   Set ID: 32  Batch ID: 102 | Loss: 0.09568\n",
            "Epoch ID: 3   Set ID: 32  Batch ID: 103 | Loss: 0.09790\n",
            "Epoch ID: 3   Set ID: 32  Batch ID: 104 | Loss: 0.09693\n",
            "Epoch ID: 3   Set ID: 11  Batch ID: 105 | Loss: 0.09637\n",
            "Epoch ID: 3   Set ID: 11  Batch ID: 106 | Loss: 0.09596\n",
            "Epoch ID: 3   Set ID: 11  Batch ID: 107 | Loss: 0.09554\n",
            "Epoch ID: 3   Set ID: 11  Batch ID: 108 | Loss: 0.09666\n",
            "Epoch ID: 3   Set ID: 45  Batch ID: 109 | Loss: 0.09512\n",
            "Epoch ID: 3   Set ID: 45  Batch ID: 110 | Loss: 0.09602\n",
            "Epoch ID: 3   Set ID: 45  Batch ID: 111 | Loss: 0.09363\n",
            "Epoch ID: 3   Set ID: 45  Batch ID: 112 | Loss: 0.09422\n",
            "Epoch ID: 3   Set ID: 64  Batch ID: 113 | Loss: 0.09504\n",
            "Epoch ID: 3   Set ID: 64  Batch ID: 114 | Loss: 0.09730\n",
            "Epoch ID: 3   Set ID: 64  Batch ID: 115 | Loss: 0.09381\n",
            "Epoch ID: 3   Set ID: 64  Batch ID: 116 | Loss: 0.09326\n",
            "Epoch ID: 3   Set ID: 29  Batch ID: 117 | Loss: 0.09890\n",
            "Epoch ID: 3   Set ID: 29  Batch ID: 118 | Loss: 0.09693\n",
            "Epoch ID: 3   Set ID: 29  Batch ID: 119 | Loss: 0.09579\n",
            "Epoch ID: 3   Set ID: 29  Batch ID: 120 | Loss: 0.09463\n",
            "Epoch ID: 3   Set ID: 5  Batch ID: 121 | Loss: 0.09335\n",
            "Epoch ID: 3   Set ID: 5  Batch ID: 122 | Loss: 0.09515\n",
            "Epoch ID: 3   Set ID: 5  Batch ID: 123 | Loss: 0.09511\n",
            "Epoch ID: 3   Set ID: 5  Batch ID: 124 | Loss: 0.09752\n",
            "Epoch ID: 3   Set ID: 53  Batch ID: 125 | Loss: 0.09646\n",
            "Epoch ID: 3   Set ID: 53  Batch ID: 126 | Loss: 0.09533\n",
            "Epoch ID: 3   Set ID: 53  Batch ID: 127 | Loss: 0.09236\n",
            "Epoch ID: 3   Set ID: 53  Batch ID: 128 | Loss: 0.09574\n",
            "Epoch ID: 3   Set ID: 12  Batch ID: 129 | Loss: 0.09511\n",
            "Epoch ID: 3   Set ID: 12  Batch ID: 130 | Loss: 0.09621\n",
            "Epoch ID: 3   Set ID: 12  Batch ID: 131 | Loss: 0.09703\n",
            "Epoch ID: 3   Set ID: 12  Batch ID: 132 | Loss: 0.09679\n",
            "Epoch ID: 3   Set ID: 39  Batch ID: 133 | Loss: 0.09943\n",
            "Epoch ID: 3   Set ID: 39  Batch ID: 134 | Loss: 0.09780\n",
            "Epoch ID: 3   Set ID: 39  Batch ID: 135 | Loss: 0.09732\n",
            "Epoch ID: 3   Set ID: 39  Batch ID: 136 | Loss: 0.09453\n",
            "Epoch ID: 3   Set ID: 50  Batch ID: 137 | Loss: 0.09672\n",
            "Epoch ID: 3   Set ID: 50  Batch ID: 138 | Loss: 0.09603\n",
            "Epoch ID: 3   Set ID: 50  Batch ID: 139 | Loss: 0.09428\n",
            "Epoch ID: 3   Set ID: 50  Batch ID: 140 | Loss: 0.09850\n",
            "Epoch ID: 3   Set ID: 24  Batch ID: 141 | Loss: 0.09602\n",
            "Epoch ID: 3   Set ID: 24  Batch ID: 142 | Loss: 0.09551\n",
            "Epoch ID: 3   Set ID: 24  Batch ID: 143 | Loss: 0.09605\n",
            "Epoch ID: 3   Set ID: 24  Batch ID: 144 | Loss: 0.09493\n",
            "Epoch ID: 3   Set ID: 25  Batch ID: 145 | Loss: 0.09412\n",
            "Epoch ID: 3   Set ID: 25  Batch ID: 146 | Loss: 0.09478\n",
            "Epoch ID: 3   Set ID: 25  Batch ID: 147 | Loss: 0.09648\n",
            "Epoch ID: 3   Set ID: 25  Batch ID: 148 | Loss: 0.09558\n",
            "Epoch ID: 3   Set ID: 71  Batch ID: 149 | Loss: 0.09836\n",
            "Epoch ID: 3   Set ID: 71  Batch ID: 150 | Loss: 0.09575\n",
            "Epoch ID: 3   Set ID: 71  Batch ID: 151 | Loss: 0.09694\n",
            "Epoch ID: 3   Set ID: 71  Batch ID: 152 | Loss: 0.09625\n",
            "Epoch ID: 3   Set ID: 1  Batch ID: 153 | Loss: 0.09586\n",
            "Epoch ID: 3   Set ID: 1  Batch ID: 154 | Loss: 0.09492\n",
            "Epoch ID: 3   Set ID: 1  Batch ID: 155 | Loss: 0.09495\n",
            "Epoch ID: 3   Set ID: 1  Batch ID: 156 | Loss: 0.09376\n",
            "Epoch ID: 3   Set ID: 66  Batch ID: 157 | Loss: 0.09677\n",
            "Epoch ID: 3   Set ID: 66  Batch ID: 158 | Loss: 0.09633\n",
            "Epoch ID: 3   Set ID: 66  Batch ID: 159 | Loss: 0.09750\n",
            "Epoch ID: 3   Set ID: 66  Batch ID: 160 | Loss: 0.09692\n",
            "Epoch ID: 3   Set ID: 61  Batch ID: 161 | Loss: 0.09523\n",
            "Epoch ID: 3   Set ID: 61  Batch ID: 162 | Loss: 0.09635\n",
            "Epoch ID: 3   Set ID: 61  Batch ID: 163 | Loss: 0.09769\n",
            "Epoch ID: 3   Set ID: 61  Batch ID: 164 | Loss: 0.09454\n",
            "Epoch ID: 3   Set ID: 28  Batch ID: 165 | Loss: 0.09432\n",
            "Epoch ID: 3   Set ID: 28  Batch ID: 166 | Loss: 0.09359\n",
            "Epoch ID: 3   Set ID: 28  Batch ID: 167 | Loss: 0.09707\n",
            "Epoch ID: 3   Set ID: 28  Batch ID: 168 | Loss: 0.09447\n",
            "Epoch ID: 3   Set ID: 73  Batch ID: 169 | Loss: 0.09408\n",
            "Epoch ID: 3   Set ID: 73  Batch ID: 170 | Loss: 0.09578\n",
            "Epoch ID: 3   Set ID: 73  Batch ID: 171 | Loss: 0.09747\n",
            "Epoch ID: 3   Set ID: 73  Batch ID: 172 | Loss: 0.09176\n",
            "Epoch ID: 3   Set ID: 23  Batch ID: 173 | Loss: 0.09708\n",
            "Epoch ID: 3   Set ID: 23  Batch ID: 174 | Loss: 0.09261\n",
            "Epoch ID: 3   Set ID: 23  Batch ID: 175 | Loss: 0.09694\n",
            "Epoch ID: 3   Set ID: 23  Batch ID: 176 | Loss: 0.09601\n",
            "Epoch ID: 3   Set ID: 10  Batch ID: 177 | Loss: 0.09616\n",
            "Epoch ID: 3   Set ID: 10  Batch ID: 178 | Loss: 0.09612\n",
            "Epoch ID: 3   Set ID: 10  Batch ID: 179 | Loss: 0.09636\n",
            "Epoch ID: 3   Set ID: 10  Batch ID: 180 | Loss: 0.09271\n",
            "Epoch ID: 3   Set ID: 30  Batch ID: 181 | Loss: 0.09418\n",
            "Epoch ID: 3   Set ID: 30  Batch ID: 182 | Loss: 0.09284\n",
            "Epoch ID: 3   Set ID: 30  Batch ID: 183 | Loss: 0.09625\n",
            "Epoch ID: 3   Set ID: 30  Batch ID: 184 | Loss: 0.09823\n",
            "Epoch ID: 3   Set ID: 13  Batch ID: 185 | Loss: 0.09392\n",
            "Epoch ID: 3   Set ID: 13  Batch ID: 186 | Loss: 0.09672\n",
            "Epoch ID: 3   Set ID: 13  Batch ID: 187 | Loss: 0.09540\n",
            "Epoch ID: 3   Set ID: 13  Batch ID: 188 | Loss: 0.09834\n",
            "Epoch ID: 3   Set ID: 67  Batch ID: 189 | Loss: 0.09447\n",
            "Epoch ID: 3   Set ID: 67  Batch ID: 190 | Loss: 0.09688\n",
            "Epoch ID: 3   Set ID: 67  Batch ID: 191 | Loss: 0.09417\n",
            "Epoch ID: 3   Set ID: 67  Batch ID: 192 | Loss: 0.09768\n",
            "Epoch ID: 3   Set ID: 2  Batch ID: 193 | Loss: 0.09524\n",
            "Epoch ID: 3   Set ID: 2  Batch ID: 194 | Loss: 0.09674\n",
            "Epoch ID: 3   Set ID: 2  Batch ID: 195 | Loss: 0.09423\n",
            "Epoch ID: 3   Set ID: 2  Batch ID: 196 | Loss: 0.09592\n",
            "Epoch ID: 3   Set ID: 7  Batch ID: 197 | Loss: 0.09728\n",
            "Epoch ID: 3   Set ID: 7  Batch ID: 198 | Loss: 0.09653\n",
            "Epoch ID: 3   Set ID: 7  Batch ID: 199 | Loss: 0.09345\n",
            "Epoch ID: 3   Set ID: 7  Batch ID: 200 | Loss: 0.09715\n",
            "Epoch ID: 3   Set ID: 47  Batch ID: 201 | Loss: 0.09620\n",
            "Epoch ID: 3   Set ID: 47  Batch ID: 202 | Loss: 0.09548\n",
            "Epoch ID: 3   Set ID: 47  Batch ID: 203 | Loss: 0.09528\n",
            "Epoch ID: 3   Set ID: 47  Batch ID: 204 | Loss: 0.09725\n",
            "Epoch ID: 3   Set ID: 41  Batch ID: 205 | Loss: 0.09597\n",
            "Epoch ID: 3   Set ID: 41  Batch ID: 206 | Loss: 0.09478\n",
            "Epoch ID: 3   Set ID: 41  Batch ID: 207 | Loss: 0.09591\n",
            "Epoch ID: 3   Set ID: 41  Batch ID: 208 | Loss: 0.09542\n",
            "Epoch ID: 3   Set ID: 78  Batch ID: 209 | Loss: 0.09533\n",
            "Epoch ID: 3   Set ID: 78  Batch ID: 210 | Loss: 0.09490\n",
            "Epoch ID: 3   Set ID: 78  Batch ID: 211 | Loss: 0.09563\n",
            "Epoch ID: 3   Set ID: 78  Batch ID: 212 | Loss: 0.09556\n",
            "Epoch ID: 3   Set ID: 37  Batch ID: 213 | Loss: 0.09550\n",
            "Epoch ID: 3   Set ID: 37  Batch ID: 214 | Loss: 0.09512\n",
            "Epoch ID: 3   Set ID: 37  Batch ID: 215 | Loss: 0.09603\n",
            "Epoch ID: 3   Set ID: 37  Batch ID: 216 | Loss: 0.09452\n",
            "Epoch ID: 3   Set ID: 70  Batch ID: 217 | Loss: 0.09558\n",
            "Epoch ID: 3   Set ID: 70  Batch ID: 218 | Loss: 0.09544\n",
            "Epoch ID: 3   Set ID: 70  Batch ID: 219 | Loss: 0.09347\n",
            "Epoch ID: 3   Set ID: 70  Batch ID: 220 | Loss: 0.09589\n",
            "Epoch ID: 3   Set ID: 22  Batch ID: 221 | Loss: 0.09655\n",
            "Epoch ID: 3   Set ID: 22  Batch ID: 222 | Loss: 0.09820\n",
            "Epoch ID: 3   Set ID: 22  Batch ID: 223 | Loss: 0.09344\n",
            "Epoch ID: 3   Set ID: 22  Batch ID: 224 | Loss: 0.09714\n",
            "Epoch ID: 3   Set ID: 68  Batch ID: 225 | Loss: 0.09553\n",
            "Epoch ID: 3   Set ID: 68  Batch ID: 226 | Loss: 0.09491\n",
            "Epoch ID: 3   Set ID: 68  Batch ID: 227 | Loss: 0.09684\n",
            "Epoch ID: 3   Set ID: 68  Batch ID: 228 | Loss: 0.09471\n",
            "Epoch ID: 3   Set ID: 42  Batch ID: 229 | Loss: 0.09652\n",
            "Epoch ID: 3   Set ID: 42  Batch ID: 230 | Loss: 0.09443\n",
            "Epoch ID: 3   Set ID: 42  Batch ID: 231 | Loss: 0.09665\n",
            "Epoch ID: 3   Set ID: 42  Batch ID: 232 | Loss: 0.09614\n",
            "Epoch ID: 3   Set ID: 49  Batch ID: 233 | Loss: 0.09511\n",
            "Epoch ID: 3   Set ID: 49  Batch ID: 234 | Loss: 0.09624\n",
            "Epoch ID: 3   Set ID: 49  Batch ID: 235 | Loss: 0.09509\n",
            "Epoch ID: 3   Set ID: 49  Batch ID: 236 | Loss: 0.09771\n",
            "Epoch ID: 3   Set ID: 16  Batch ID: 237 | Loss: 0.09690\n",
            "Epoch ID: 3   Set ID: 16  Batch ID: 238 | Loss: 0.09520\n",
            "Epoch ID: 3   Set ID: 16  Batch ID: 239 | Loss: 0.09402\n",
            "Epoch ID: 3   Set ID: 16  Batch ID: 240 | Loss: 0.09673\n",
            "Epoch ID: 3   Set ID: 14  Batch ID: 241 | Loss: 0.09586\n",
            "Epoch ID: 3   Set ID: 14  Batch ID: 242 | Loss: 0.09410\n",
            "Epoch ID: 3   Set ID: 14  Batch ID: 243 | Loss: 0.09654\n",
            "Epoch ID: 3   Set ID: 14  Batch ID: 244 | Loss: 0.09571\n",
            "Epoch ID: 3   Set ID: 15  Batch ID: 245 | Loss: 0.09463\n",
            "Epoch ID: 3   Set ID: 15  Batch ID: 246 | Loss: 0.09551\n",
            "Epoch ID: 3   Set ID: 15  Batch ID: 247 | Loss: 0.09238\n",
            "Epoch ID: 3   Set ID: 15  Batch ID: 248 | Loss: 0.09552\n",
            "Epoch ID: 3   Set ID: 20  Batch ID: 249 | Loss: 0.09326\n",
            "Epoch ID: 3   Set ID: 20  Batch ID: 250 | Loss: 0.09713\n",
            "Epoch ID: 3   Set ID: 20  Batch ID: 251 | Loss: 0.09523\n",
            "Epoch ID: 3   Set ID: 20  Batch ID: 252 | Loss: 0.09586\n",
            "Epoch ID: 3   Set ID: 3  Batch ID: 253 | Loss: 0.09703\n",
            "Epoch ID: 3   Set ID: 3  Batch ID: 254 | Loss: 0.09581\n",
            "Epoch ID: 3   Set ID: 3  Batch ID: 255 | Loss: 0.09324\n",
            "Epoch ID: 3   Set ID: 3  Batch ID: 256 | Loss: 0.09555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "a=torch.tensor(cost)\n",
        "cost1=a.cpu().numpy()\n",
        "plt.plot(cost1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "gCjuk6Wkr3yA",
        "outputId": "401589d0-ccbb-4abd-b253-a0032b97b91f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f690e422280>]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model): \n",
        "  \n",
        "  \n",
        "    loss_val = [] \n",
        "    eff = ValueSet(0, 0, 0, 0) \n",
        "    # switch to evaluate mode \n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for setID in val_set_idx: \n",
        "          val_set = MyDataset(setID+1) \n",
        "          val_generator = torch.utils.data.DataLoader(val_set,  \n",
        "                                                      batch_size=5000,  \n",
        "                                                      shuffle=True) \n",
        "          print(setID) \n",
        "          for X_val, y_val in val_generator: \n",
        "            X_val=X_val.to(DEVICE)\n",
        "            y_val=y_val.to(DEVICE)\n",
        "            # Forward pass \n",
        "            val_outputs = model(X_val) \n",
        "            loss_output = loss_model.forward(val_outputs, y_val) \n",
        "            loss_val.append(loss_output) \n",
        "            for label, output in zip(y_val.cpu().numpy(), val_outputs.cpu().numpy()):\n",
        "                eff += efficiency(label, output, difference = 5.0,  \n",
        "                                  threshold = 1e-2, integral_threshold = 0.2,  \n",
        "                                  min_width = 3) \n",
        "    return sum(loss_val)/len(loss_val), eff.eff_rate, eff.fp_rate"
      ],
      "metadata": {
        "id": "V7dDt-LNGysO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_val, eff_rate, fp_rate = validate(model) \n",
        "print('Loss: %0.3f ' % loss_val, end=\"\")\n",
        "print('  Efficiency: %0.3f' % eff_rate, end=\"\") \n",
        "print('  False positive rate: %0.3f' % fp_rate) \n"
      ],
      "metadata": {
        "id": "K9ufnmVhf1IT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e37294bc-4a9c-42a3-dd3d-022f641b9e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54\n",
            "72\n",
            "52\n",
            "77\n",
            "63\n",
            "58\n",
            "74\n",
            "6\n",
            "18\n",
            "46\n",
            "65\n",
            "33\n",
            "38\n",
            "48\n",
            "19\n",
            "Loss: 0.096   Efficiency: 0.000  False positive rate: 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9eotz5JSaYI",
        "outputId": "16074c8f-9a7f-44a7-b8f8-9db8d1616cac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Resnet10(\n",
              "  (block_1): Sequential(\n",
              "    (0): Conv1d(1, 2, kernel_size=(2,), stride=(1,))\n",
              "    (1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv1d(2, 1, kernel_size=(1,), stride=(1,), padding=(1,))\n",
              "    (4): Trim()\n",
              "    (5): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (block_2): Sequential(\n",
              "    (0): Conv1d(1, 2, kernel_size=(2,), stride=(1,))\n",
              "    (1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv1d(2, 1, kernel_size=(2,), stride=(1,), padding=(1,))\n",
              "    (4): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (block_3): Sequential(\n",
              "    (0): Conv1d(1, 2, kernel_size=(1,), stride=(1,))\n",
              "    (1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Conv1d(2, 1, kernel_size=(2,), stride=(1,), padding=(1,))\n",
              "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "\n",
        " \n",
        "  \n",
        "# creating the dataset\n",
        "data = {'Train Loss':0.08616, 'Test loss':0.096}\n",
        "TrainTest = list(data.keys())\n",
        "values = list(data.values())\n",
        "  \n",
        "fig = plt.figure(figsize = (10, 5))\n",
        " \n",
        "# creating the bar plot\n",
        "plt.bar(TrainTest, values, color ='maroon',\n",
        "        width = 0.4)\n",
        " \n",
        "plt.xlabel(\"Dataset\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Measure of overfitting and underfitting\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "XWlYFc5HqYI7",
        "outputId": "9c09df3d-b95a-42d6-9449-19d453676824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Saving the model"
      ],
      "metadata": {
        "id": "5NuHgfKzQZGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(),'M14993314_model.pt')"
      ],
      "metadata": {
        "id": "7zzU2uLzeh0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2=Resnet10()\n",
        "model3=model2.to(DEVICE)\n",
        "loss_model = Loss(0.00001)"
      ],
      "metadata": {
        "id": "gRBKvGyyewss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the model"
      ],
      "metadata": {
        "id": "fqiIqEmcQecn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model3.load_state_dict(torch.load('M14993314_model.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BTBDPBHe15-",
        "outputId": "85598220-6b44-4014-f07e-6a2b2498d6a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model): \n",
        "  \n",
        "    loss_val = [] \n",
        "    eff = ValueSet(0, 0, 0, 0) \n",
        "    # switch to evaluate mode \n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for setID in val_set_idx: \n",
        "          val_set = MyDataset(setID+1) \n",
        "          val_generator = torch.utils.data.DataLoader(val_set,  \n",
        "                                                      batch_size=5000,  \n",
        "                                                      shuffle=True) \n",
        "          print(setID) \n",
        "          for X_val, y_val in val_generator: \n",
        "            X_val=X_val.to(DEVICE)\n",
        "            y_val=y_val.to(DEVICE)\n",
        "            # Forward pass \n",
        "            val_outputs = model(X_val) \n",
        "            loss_output = loss_model.forward(val_outputs, y_val) \n",
        "            loss_val.append(loss_output) \n",
        "            for label, output in zip(y_val.cpu().numpy(), val_outputs.cpu().numpy()):\n",
        "                eff += efficiency(label, output, difference = 5.0,  \n",
        "                                  threshold = 1e-2, integral_threshold = 0.2,  \n",
        "                                  min_width = 3) \n",
        "    return sum(loss_val)/len(loss_val), eff.eff_rate, eff.fp_rate"
      ],
      "metadata": {
        "id": "g_VV0jmEe6pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_val, eff_rate, fp_rate = validate(model=model2) \n",
        "print('Loss: %0.3f ' % loss_val, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrPU6UWdfCLZ",
        "outputId": "7fdef4e0-81a1-44fc-a553-b690638cb4d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54\n",
            "72\n",
            "52\n",
            "77\n",
            "63\n",
            "58\n",
            "74\n",
            "6\n",
            "18\n",
            "46\n",
            "65\n",
            "33\n",
            "38\n",
            "48\n",
            "19\n",
            "Loss: 0.096 "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checking the impact of sample size on validation set loss"
      ],
      "metadata": {
        "id": "wq8rLXoDQhmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model,train_set_idx, num_epochs,\n",
        "          learning_rate=0.01, seed=123, batch_size=128):\n",
        "    cost = []\n",
        "    \n",
        "    torch.manual_seed(seed)\n",
        "    \n",
        "    optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    for e in range(1,num_epochs):\n",
        "      batch_num = 0\n",
        "      for setID in train_set_idx:\n",
        "          train_set = MyDataset(setID+1)\n",
        "          train_generator = torch.utils.data.DataLoader(train_set, \n",
        "                                                        batch_size=batch_size, \n",
        "      shuffle=True)\n",
        "          print(setID)\n",
        "          for X_train, y_train in train_generator:\n",
        "            X_train=X_train.to(DEVICE)\n",
        "            y_train=y_train.to(DEVICE)\n",
        "            batch_num = batch_num + 1\n",
        "            #### Compute outputs ####\n",
        "            yhat = model(X_train)\n",
        "            loss = loss_model.forward(yhat, y_train)\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            #### Compute gradients ####\n",
        "            loss.backward()\n",
        "            \n",
        "            #### Update weights ####\n",
        "            optimizer.step()\n",
        "            #### Logging ####\n",
        "            with torch.no_grad():\n",
        "                yhat = model.forward(X_train)\n",
        "                curr_loss = loss_model.forward(yhat, y_train)\n",
        "                cost.append(curr_loss)\n",
        "    return cost"
      ],
      "metadata": {
        "id": "W5wtaPIPiZkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model,val_set_idx):\n",
        "    loss_val = []\n",
        "    eff = ValueSet(0, 0, 0, 0)\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for setID in val_set_idx:\n",
        "          val_set = MyDataset(setID+1)\n",
        "          val_generator = torch.utils.data.DataLoader(val_set, \n",
        "                                                      batch_size=1024, \n",
        "    shuffle=True)\n",
        "          # print(setID)\n",
        "          for X_val, y_val in val_generator:\n",
        "            X_val=X_val.to(DEVICE)\n",
        "            y_val=y_val.to(DEVICE)\n",
        "            # Forward pass\n",
        "            val_outputs = model(X_val)\n",
        "            loss_output = loss_model.forward(val_outputs, y_val)\n",
        "            loss_val.append(loss_output)\n",
        "            for label, output in zip(y_val.cpu().numpy(), val_outputs.cpu().numpy()):\n",
        "                eff += efficiency(label, output, difference = 5.0, \n",
        "                                  threshold = 1e-2, integral_threshold = 0.2, \n",
        "                                  min_width = 3)\n",
        "    return sum(loss_val)/len(loss_val), eff.eff_rate, eff.fp_rate"
      ],
      "metadata": {
        "id": "MIaB1No9iOKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=[15,24,40,50]\n",
        "train_cost=[]\n",
        "test_cost=[]\n",
        "for i in a:\n",
        "  train_set_idx, val_set_idx=train_test_split(list(range(1,80)),test_size=i)\n",
        "  torch.manual_seed(123)\n",
        "  model = Resnet10()\n",
        "  model=model.to(DEVICE)\n",
        "  # loss_model = Loss(0.00001)\n",
        "  cost = train(model,train_set_idx,\n",
        "             num_epochs=3,\n",
        "             learning_rate=0.001,\n",
        "             seed=123, batch_size=128)\n",
        "  b=torch.tensor(cost)\n",
        "  cost1=b.cpu().numpy()\n",
        "  train_cost.append(cost1[-1])\n",
        "  loss_val, eff_rate, fp_rate = validate(model,val_set_idx)\n",
        "  loss_test=loss_val.cpu().numpy()\n",
        "  loss_test\n",
        "  test_cost.append(loss_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZT58hMXTiSxX",
        "outputId": "31d15f81-3033-403a-bae1-9c8126af3ed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37\n",
            "36\n",
            "79\n",
            "71\n",
            "77\n",
            "30\n",
            "55\n",
            "46\n",
            "62\n",
            "44\n",
            "67\n",
            "23\n",
            "32\n",
            "66\n",
            "24\n",
            "35\n",
            "27\n",
            "26\n",
            "39\n",
            "4\n",
            "57\n",
            "58\n",
            "38\n",
            "22\n",
            "17\n",
            "49\n",
            "76\n",
            "16\n",
            "19\n",
            "63\n",
            "42\n",
            "40\n",
            "64\n",
            "73\n",
            "47\n",
            "5\n",
            "53\n",
            "60\n",
            "70\n",
            "43\n",
            "6\n",
            "9\n",
            "50\n",
            "45\n",
            "15\n",
            "41\n",
            "56\n",
            "33\n",
            "13\n",
            "54\n",
            "65\n",
            "75\n",
            "72\n",
            "10\n",
            "29\n",
            "28\n",
            "1\n",
            "21\n",
            "59\n",
            "61\n",
            "68\n",
            "18\n",
            "20\n",
            "74\n",
            "37\n",
            "36\n",
            "79\n",
            "71\n",
            "77\n",
            "30\n",
            "55\n",
            "46\n",
            "62\n",
            "44\n",
            "67\n",
            "23\n",
            "32\n",
            "66\n",
            "24\n",
            "35\n",
            "27\n",
            "26\n",
            "39\n",
            "4\n",
            "57\n",
            "58\n",
            "38\n",
            "22\n",
            "17\n",
            "49\n",
            "76\n",
            "16\n",
            "19\n",
            "63\n",
            "42\n",
            "40\n",
            "64\n",
            "73\n",
            "47\n",
            "5\n",
            "53\n",
            "60\n",
            "70\n",
            "43\n",
            "6\n",
            "9\n",
            "50\n",
            "45\n",
            "15\n",
            "41\n",
            "56\n",
            "33\n",
            "13\n",
            "54\n",
            "65\n",
            "75\n",
            "72\n",
            "10\n",
            "29\n",
            "28\n",
            "1\n",
            "21\n",
            "59\n",
            "61\n",
            "68\n",
            "18\n",
            "20\n",
            "74\n",
            "72\n",
            "39\n",
            "15\n",
            "5\n",
            "29\n",
            "65\n",
            "16\n",
            "66\n",
            "55\n",
            "25\n",
            "79\n",
            "68\n",
            "33\n",
            "44\n",
            "64\n",
            "12\n",
            "34\n",
            "41\n",
            "78\n",
            "32\n",
            "69\n",
            "63\n",
            "4\n",
            "6\n",
            "17\n",
            "48\n",
            "56\n",
            "23\n",
            "8\n",
            "10\n",
            "74\n",
            "37\n",
            "3\n",
            "22\n",
            "50\n",
            "35\n",
            "1\n",
            "71\n",
            "61\n",
            "19\n",
            "43\n",
            "36\n",
            "28\n",
            "24\n",
            "58\n",
            "67\n",
            "51\n",
            "21\n",
            "60\n",
            "40\n",
            "14\n",
            "18\n",
            "9\n",
            "11\n",
            "42\n",
            "72\n",
            "39\n",
            "15\n",
            "5\n",
            "29\n",
            "65\n",
            "16\n",
            "66\n",
            "55\n",
            "25\n",
            "79\n",
            "68\n",
            "33\n",
            "44\n",
            "64\n",
            "12\n",
            "34\n",
            "41\n",
            "78\n",
            "32\n",
            "69\n",
            "63\n",
            "4\n",
            "6\n",
            "17\n",
            "48\n",
            "56\n",
            "23\n",
            "8\n",
            "10\n",
            "74\n",
            "37\n",
            "3\n",
            "22\n",
            "50\n",
            "35\n",
            "1\n",
            "71\n",
            "61\n",
            "19\n",
            "43\n",
            "36\n",
            "28\n",
            "24\n",
            "58\n",
            "67\n",
            "51\n",
            "21\n",
            "60\n",
            "40\n",
            "14\n",
            "18\n",
            "9\n",
            "11\n",
            "42\n",
            "75\n",
            "55\n",
            "24\n",
            "32\n",
            "35\n",
            "64\n",
            "37\n",
            "47\n",
            "41\n",
            "38\n",
            "57\n",
            "62\n",
            "49\n",
            "28\n",
            "9\n",
            "22\n",
            "25\n",
            "26\n",
            "5\n",
            "8\n",
            "21\n",
            "23\n",
            "15\n",
            "67\n",
            "1\n",
            "59\n",
            "48\n",
            "51\n",
            "42\n",
            "63\n",
            "53\n",
            "7\n",
            "52\n",
            "61\n",
            "6\n",
            "18\n",
            "13\n",
            "16\n",
            "14\n",
            "75\n",
            "55\n",
            "24\n",
            "32\n",
            "35\n",
            "64\n",
            "37\n",
            "47\n",
            "41\n",
            "38\n",
            "57\n",
            "62\n",
            "49\n",
            "28\n",
            "9\n",
            "22\n",
            "25\n",
            "26\n",
            "5\n",
            "8\n",
            "21\n",
            "23\n",
            "15\n",
            "67\n",
            "1\n",
            "59\n",
            "48\n",
            "51\n",
            "42\n",
            "63\n",
            "53\n",
            "7\n",
            "52\n",
            "61\n",
            "6\n",
            "18\n",
            "13\n",
            "16\n",
            "14\n",
            "64\n",
            "45\n",
            "24\n",
            "58\n",
            "36\n",
            "6\n",
            "63\n",
            "10\n",
            "48\n",
            "13\n",
            "51\n",
            "19\n",
            "38\n",
            "75\n",
            "29\n",
            "70\n",
            "37\n",
            "62\n",
            "57\n",
            "50\n",
            "47\n",
            "18\n",
            "28\n",
            "55\n",
            "31\n",
            "2\n",
            "34\n",
            "65\n",
            "35\n",
            "64\n",
            "45\n",
            "24\n",
            "58\n",
            "36\n",
            "6\n",
            "63\n",
            "10\n",
            "48\n",
            "13\n",
            "51\n",
            "19\n",
            "38\n",
            "75\n",
            "29\n",
            "70\n",
            "37\n",
            "62\n",
            "57\n",
            "50\n",
            "47\n",
            "18\n",
            "28\n",
            "55\n",
            "31\n",
            "2\n",
            "34\n",
            "65\n",
            "35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = ['15','24','40','50']\n",
        "f = train_cost\n",
        "g = test_cost\n",
        "  \n",
        "X_axis = np.arange(len(X))\n",
        "  \n",
        "plt.bar(X_axis - 0.2, f, 0.4, label = 'Train loss')\n",
        "plt.bar(X_axis + 0.2, g, 0.4, label = 'Test loss')\n",
        "  \n",
        "plt.xticks(X_axis, X)\n",
        "plt.xlabel(\"Test sample size\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Impact of sample size on validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "E1YB-U9JTJyB",
        "outputId": "3bb8bb21-d6ac-4bc0-d1b0-1e37874bdfc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}